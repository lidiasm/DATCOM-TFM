{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Regresión Logística\n",
    "\n",
    "## 1. Introducción a la Regresión Logística\n",
    "\n",
    "Se trata de un **algoritmo estadístico** cuyo objetivo es aproximar una **función matemática acotada** en el intervalo [0, 1]. Así permite la estimación de las **probabilidades de pertenencia** de cada muestra a las distintas clases. Como la detección de sexismo es un problema de clasificación binario, se establece el umbral en 0.5 con el que asignar una clase a cada instancia aplicando las siguientes casuísticas:\n",
    "\n",
    "* Si la probabilidad es **menor que 0.5** entonces la clase es **no sexista** (categoría 0). \n",
    "* Si la probabilidad es **mayor que 0.5** entonces la clase es **sexista** (categoría 1).\n",
    "\n",
    "### 1.1. Condiciones de uso\n",
    "\n",
    "* **Mínima correlación** entre los predictores o variables independientes.\n",
    "* La existencia de una **relación lineal** entre los predictores y la probabilidad de cada variable dependiente.\n",
    "* Mínima existencia de **outliers**.\n",
    "* Conjunto de datos razonáblemente amplio.\n",
    "\n",
    "### 1.2. Casos de uso\n",
    "\n",
    "1. Construir **modelos lineales** para solventar problemas de clasificación cuyos datos son fácilmente separables.\n",
    "\n",
    "2. Comprobar de la existencia de **relaciones lineales** entre los predictores y la variable dependiente en función de los valores de los coeficientes del hiperplano.\n",
    "\n",
    "   - Si es 0 significa que **no existe una relación lineal** entre un predictor y la variable dependiente por lo que dicho predictor no ayuda en su predicción.\n",
    "\n",
    "   - Si es mayor que 0 supone un crecimiento simultáneo del predictor al aumentar la variable dependiente, mientras que si es menor que 0 simboliza el incremento del predictor cuando disminuye la variable dependiente. Aunque en ambos casos el **predictor puede ser útil para predecir** la clase de una muestra."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructura del notebook\n",
    "\n",
    "1. Introducción a la Regresión Logística\n",
    "2. Estructura del notebook\n",
    "3. Instalación y carga de librerías\n",
    "4. Lectura y carga de datos\n",
    "5. Técnicas de codificación de textos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instalación y carga de librerías\n",
    "\n",
    "Este apartado tiene como único propósito cargar las librerías y dependencias necesarias para la ejecución de este notebook, así como las funciones propiamente desarrolladas. Previo a ello deberán ser instaladas bien ejecutando el script *setup.sh* mediante el comando `bash setup.sh` con permisos de ejecución en distribuciones Linux, o bien ejecutando el compando `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lidia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# Import data read and compute functions\n",
    "from data import read_training_dataset, read_testing_dataset\n",
    "\n",
    "# Import text preprocess functions\n",
    "from processing import *\n",
    "\n",
    "# Import LR models and validation metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lectura y carga de datos\n",
    "\n",
    "En esta sección se pretende **cargar los datasets de entrenamiento y validación** procedentes de los correspondientes ficheros situados en la carpeta *data*. Al tener un **formato TSV** se deben leer como tablas aunque posteriormente se trabaje con ellos en formato *dataframe*. \n",
    "\n",
    "Tal y como se puede comprobar en los siguientes resultados las dimensiones de sendos conjuntos de datos se detallan a continuación:\n",
    "\n",
    "* Conjunto de entrenamiento: **6.977 muestras**.\n",
    "* Conjunto de validación: **4.368 muestras**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions: (6977, 7)\n",
      "Testing dataset dimensions: (4368, 7)\n"
     ]
    }
   ],
   "source": [
    "# Read EXIST datasets\n",
    "training_df = read_training_dataset()\n",
    "testing_df = read_testing_dataset()\n",
    "\n",
    "# Show the dimensions of the datasets\n",
    "print(\"Training dataset dimensions:\", training_df.shape)\n",
    "print(\"Testing dataset dimensions:\", testing_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Técnicas de codificación de textos\n",
    "\n",
    "### 5.1. Bolsa de palabras\n",
    "\n",
    "Es una técnica orientada a la **extracción de características** basado en **texto** que convierte cada documento en un vector de longitud fija a partir del cálculo de la frecuencia de sus términos. Si bien se trata de un procedimiento altamente sencillo y rápido de aplicar, **no** respeta el **orden de los términos** en los documentos y tampoco se considera el **contexto** de los mismos como consecuencia.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experimentos y modelos\n",
    "\n",
    "A continuación se detallan los experimentos más relevantes de entre las distintas pruebas realizadas mediante diversas combinaciones de técnicas de procesamiento de textos, codificación de documentos y algoritmos clásicos de Aprendizaje Automático.\n",
    "\n",
    "### 6.1. Primer experimento\n",
    "\n",
    "- Procesamiento de los conjuntos de entrenamiento y validación, incluyendo las siguientes técnicas:\n",
    "\n",
    "  - Elimina caracteres especiales, no alfabéticos y signos de puntuación.\n",
    "  - Elimina hashtags y menciones de usuarios.\n",
    "  - Elimina *stopwords* de los documentos en inglés y español.\n",
    "  - Convierte todos los caracteres en minúsculas.\n",
    "\n",
    "- Codificación de la variable dependiente *task1* a etiquetas numéricas en los conjuntos de entrenamiento y validación, estableciendo la asignación *non-sexist*~0 y *sexist*~1.\n",
    "\n",
    "- Generación de una bolsa de palabras para entrenamiento y otra para validación como codificación de documentos en valores numéricos.\n",
    "\n",
    "- Entrenamiento, hiperparametrización y validación con Regresión Logística para la construcción de varios modelos, siendo la siguiente configuración la que mejor resultados de validación ha conseguido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.689\n"
     ]
    }
   ],
   "source": [
    "# Text processing in training and testing datasets\n",
    "training_df = text_processing_pipeline(training_df, \"text\")\n",
    "testing_df = text_processing_pipeline(testing_df, \"text\")\n",
    "\n",
    "# Encode the class labels to numeric values for training and testing datasets providing the desirable assignation\n",
    "encoding = {\"non-sexist\": 0, \"sexist\": 1}\n",
    "encoded_training_labels = encode_to_numeric_labels(training_df, \"task1\", encoding)\n",
    "encoded_testing_labels = encode_to_numeric_labels(testing_df, \"task1\", encoding)\n",
    "\n",
    "# Train a CountVectorizer object to encode texts as bag of words for training and testing datasets\n",
    "bag_words = to_bag_of_words(list(training_df[\"cleaned_text\"].values), \n",
    "                            list(testing_df[\"cleaned_text\"].values))\n",
    "\n",
    "# Create and configure a LR model\n",
    "lr = LogisticRegression(random_state=1, solver='lbfgs', multi_class='ovr')\n",
    "\n",
    "# Train the LR model\n",
    "lr.fit(bag_words[\"training\"], encoded_training_labels)\n",
    "\n",
    "# Predict the testing labels with the trained LR model\n",
    "predictions = lr.predict(bag_words[\"testing\"])\n",
    "\n",
    "# LR model validation\n",
    "print(\"Accuracy %.3f\" %metrics.accuracy_score(encoded_testing_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00f7dc61815a6da5453fee0a1d7c3baaa88d552412e55cf65ecdf10d17265d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
