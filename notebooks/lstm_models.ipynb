{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos LSTM\n",
    "\n",
    "## 1. Introducción a LSTM\n",
    "\n",
    "LSTM es un acrónimo de *Long-Short Term Memory* y representa a un **subtipo de RNN** (*Recurrent Neural Network*) capaz de **retener información relevante** sobre datos ya procesados que ayude al procesamiento de nuevas secuencias de datos completas. Su arquitectura se encuentra compuesta a su vez por tres redes neuronales:\n",
    "\n",
    "* ***Forget Gate***: este primer modelo es el encargado de filtrar qué información previa es útil para su almacenamiento y qué datos ya no son útiles para futuras iteraciones. \n",
    "\n",
    "* ***Input Gate***: esta segunda red trata de determinar el valor que presentan los datos entrantes para resolver la tarea de clasificación.\n",
    "\n",
    "* ***Output Gate***: finalmente esta red calcula las salidas del modelo LSTM que dependerán de la tarea de clasificación que se pretende abordar.\n",
    "\n",
    "### 1.1. Introducción a BiLSTM\n",
    "\n",
    "Se trata de una variante de la arquitectura compuesta por **dos redes LSTM independientes** con el objetivo de procesar los textos de derecha a izquierda y viceversa. Esta característica permite la **extracción de características en ambos sentidos** proporcionando un **contexto más voluminoso y preciso** al considerar los **términos precedores y sucesores**, almacenando así información pasada y futura del texto. Así, generalmente las redes BiLSTM tienden a mejorar el rendimiento y su capacidad preditiva.\n",
    "\n",
    "### 1.2. Condiciones de uso\n",
    "\n",
    "Dependiendo del framework que se pretenda utilizar (Tensorflow, Keras, Pytorch) existen diferentes tratamientos de datos y requisitos de implementación que se deben cumplir al definir la arquitectura, entrenamiento y validación de modelos. En mi caso particular he optado por utilizar **Keras** debido a la experiencia previa que tengo con la librería y a su facilidad de uso. \n",
    "\n",
    "1. **Procesamiento y limpieza** de los documentos.\n",
    "\n",
    "2. **Tokenización** de los documentos especificando un token para aquellos términos que no sean reconocidos dentro de un vocabulario de palabras.\n",
    "\n",
    "3. **Codificación** numérica en forma de matrices secuenciales de valores. \n",
    "\n",
    "5. **Normalización** de las secuencias numéricas para establecer un mismo **tamaño fijo**, completando con ceros aquellas de menor longitud y separando en varias secuencias aquellas que dispongan de un mayor tamaño.\n",
    "\n",
    "6. Definición de la arquitectura de un **modelo** e instanciación para su posterior entrenamiento y validación.\n",
    "\n",
    "### 1.3. Casos de uso\n",
    "\n",
    "* Detección y extracción de patrones en secuencias de datos.\n",
    "* Modelado del lenguaje natural.\n",
    "* Traducción de texto.\n",
    "* Reconocimiento de textos manuscritos.\n",
    "* Generación de imágenes mediante mecanismos de atención.\n",
    "* Sistemas de preguntas y respuestas.\n",
    "* Conversión de vídeo a texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructura del notebook\n",
    "\n",
    "1. Introducción a LSTM\n",
    "2. Estructura del notebook\n",
    "3. Instalación y carga de librerías\n",
    "4. Lectura y carga de datos\n",
    "5. Experimentos y modelos\n",
    "6. Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instalación y carga de librerías\n",
    "\n",
    "Este apartado tiene como único propósito cargar las librerías y dependencias necesarias para la ejecución de este notebook, así como las funciones propiamente desarrolladas. Previo a ello deberán ser instaladas bien ejecutando el script *setup.sh* mediante el comando `bash setup.sh` con permisos de ejecución en distribuciones Linux, o bien ejecutando el compando `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 19:30:36.484698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 19:30:36.928273: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-21 19:30:36.928325: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-21 19:30:38.083702: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 19:30:38.083775: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-21 19:30:38.083782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-21 19:30:40.565987: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-21 19:30:40.566215: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-21 19:30:40.566230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lidiasm): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "# Import data read and compute functions\n",
    "from data import read_train_dataset, read_test_dataset\n",
    "\n",
    "# Import text preprocess functions\n",
    "from processing import *\n",
    "\n",
    "# numpy: to work with numeric codifications and embeddings\n",
    "import numpy as np\n",
    "\n",
    "# keras: to define and build LSTM models\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import LSTM, Activation, Dense, Input, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# sklearn: to plot a confusion matrix per trained model\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# matplotlib: to plot charts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lectura y carga de datos originales\n",
    "\n",
    "En esta sección se pretende **cargar los datasets de entrenamiento y validación** procedentes de los correspondientes ficheros situados en la carpeta *data*. Al tener un **formato TSV** se deben leer como tablas aunque posteriormente se trabaje con ellos en formato *dataframe*. \n",
    "\n",
    "Tal y como se puede comprobar en los siguientes resultados las dimensiones de sendos conjuntos de datos se detallan a continuación:\n",
    "\n",
    "* Conjunto de entrenamiento: **6.977 muestras**.\n",
    "* Conjunto de validación: **4.368 muestras**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset dimensions: (6977, 7)\n",
      "Test dataset dimensions: (4368, 7)\n"
     ]
    }
   ],
   "source": [
    "# Read EXIST datasets\n",
    "train_df = read_train_dataset()\n",
    "test_df = read_test_dataset()\n",
    "\n",
    "# Show the dimensions of the datasets\n",
    "print(\"Train dataset dimensions:\", train_df.shape)\n",
    "print(\"Test dataset dimensions:\", test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentos y modelos\n",
    "\n",
    "Esta sección pretende detallar los experimentos que se realizan a través de la combinación de diferentes técnicas de procesamiento de textos, codificación de documentos y arquitecturas de modelos LSTM. Como se trata de **experimentos no determinísticos**, es decir, los resultados difieren en varias ejecuciones aún con la misma configuración, la estrategia a seguir consiste en realizar **30 iteraciones de cada experimento** para luego calcular la **media de accuracy y AUC**, las métricas de evaluación escogidas para medir la calidad de un clasificador. \n",
    "\n",
    "Por lo tanto las siguientes secciones contienen los detalles del conjunto de experimentos realizados y las conclusiones comparativas alcanzadas, incluyendo el código, la configuración y los resultados únicamente del experimento con mejor rendimiento con respecto a las métricas de evaluación mencionadas.\n",
    "\n",
    "Previo al comienzo de la experimentación se definen tres funciones comunes para el tratamiento y codificación de documentos, carga de embeddings pre-entrenados y validación de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_matrix(\n",
    "    max_n_words: int, sequence_len: int, \n",
    "    lemm: bool = False, stemm: bool = False):\n",
    "    \"\"\"\n",
    "    Process the train and test documents to then convert them\n",
    "    into numeric sequence matrixes so the datasets can be\n",
    "    used to train a LSTM model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_n_words : int\n",
    "        Maximum number of words to keep within the LSTM memory\n",
    "        based on computing the word frequency.\n",
    "    sequence_len : int\n",
    "        Maximum lenght of all sequences.\n",
    "    lemm : bool (optional)\n",
    "        True to apply lemmatization to the train and test documents.\n",
    "    stemm : bool (optional)\n",
    "        True to apply stemming to the train and test documents.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary with the following keys:\n",
    "        - 'tokenizer': a Keras Tokenizer object based on the train documents\n",
    "        that contains the vocabulary to then be used to create the embeddings.\n",
    "        - 'train_matrix', 'test_matrix': the numeric sequence matrixes\n",
    "        after converting the train and test documents.\n",
    "        - 'train_labels', 'test_labels': two numeric lists which contains\n",
    "        the encoded class labels for train and test datasets.\n",
    "    \"\"\"\n",
    "    # Process train and test text documents\n",
    "    processed_df = process_encode_datasets(\n",
    "        train_df=train_df, \n",
    "        test_df=test_df,\n",
    "        lemm=lemm, \n",
    "        stemm=stemm,\n",
    "        correct_words=False\n",
    "    )\n",
    "\n",
    "    # Processed train texts and encoded train labels \n",
    "    train_texts = list(processed_df[\"train_df\"][\"cleaned_text\"].values)\n",
    "    train_labels = processed_df[\"encoded_train_labels\"]\n",
    "\n",
    "    # Processed test texts and encoded test labels\n",
    "    test_texts = list(processed_df[\"test_df\"][\"cleaned_text\"].values)\n",
    "    test_labels = processed_df[\"encoded_test_labels\"]\n",
    "\n",
    "    # Createa a tokenizer based on train texts\n",
    "    tokenizer = Tokenizer(num_words=max_n_words)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Transform each text into a numeric sequence\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "\n",
    "    # Transform each numeric sequence into a 2D vector\n",
    "    train_matrix = pad_sequences(\n",
    "        sequences=train_sequences, \n",
    "        maxlen=sequence_len)\n",
    "\n",
    "    # Tokenize the test documents using the prior trained tokenizer\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "    # Transform each numeric sequence into a 2D vector\n",
    "    test_matrix = pad_sequences(\n",
    "        sequences=test_sequences,\n",
    "        maxlen=sequence_len)\n",
    "\n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_matrix\": train_matrix,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"test_matrix\": test_matrix,\n",
    "        \"test_labels\": test_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_file: str, tokenizer: Tokenizer, sequence_len: int):\n",
    "    \"\"\"\n",
    "    Load the embeddings stored in the provided file to then\n",
    "    create a matrix with the numeric encoding of each\n",
    "    available word within the tokenizer vocabulary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_file : str\n",
    "        The path to the file which contains a set of embeddings\n",
    "    tokenizer : Tokenizer (Keras)\n",
    "        A trained Keras tokenizer which contains the vocabulary\n",
    "        of the documents to use during the training of models\n",
    "    sequence_len : int\n",
    "        Maximum lenght of all embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Numpy ndarray which represents an embedding matrix.\n",
    "    \"\"\"\n",
    "    # Load the embeddings stored in a TXT file\n",
    "    embedding_file = open(embedding_file)\n",
    "\n",
    "    # Store each word with its embeddings\n",
    "    embeddings_index = {\n",
    "        line.split()[0]:np.asarray(line.split()[1:], dtype=\"float32\") \n",
    "        for line in embedding_file\n",
    "    }\n",
    "\n",
    "    # Initialize the embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros(shape=(len(tokenizer.word_index)+1, sequence_len))\n",
    "\n",
    "    # Complete the matrix with the prior loaded embeddings\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Search for the embeddings of each word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "        # Words not found will be zeros\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lstm_model(\n",
    "    model: Model, \n",
    "    train_matrix: np.ndarray, train_labels: list, \n",
    "    test_matrix: np.ndarray, test_labels: list,\n",
    "    metrics_filename: str = None, conf_matrix_filename: str = None):\n",
    "    \"\"\"\n",
    "    Evaluates the provided trained LSTM model over the \n",
    "    train and test datasets to get the accuracy, AUC and\n",
    "    a confusion matrix. To create the predictions for a\n",
    "    binary classification a threshold has been set:\n",
    "        - <= 0.5 represents the negative class (non-sexist).\n",
    "        - > 0.5 represents the positive class (sexist).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Keras model\n",
    "        A trained Keras model to be evaluated.\n",
    "    train_matrix : Numpy ndarray\n",
    "        A numeric sequence matrix with the trained documents.\n",
    "    train_labels : list\n",
    "        A numeric list with the class labels of the train dataset.\n",
    "    test_matrix : Numpy ndarray\n",
    "        A numeric sequence matrix with the test documents.\n",
    "    test_labels : list\n",
    "        A numeric list with the class labels of the test dataset.\n",
    "    metrics_filename : str (optional)\n",
    "        A path and filename to save the metrics over the \n",
    "        train and test datasets in a TXT file.\n",
    "    conf_matrix_filename : str (optional)\n",
    "        A path and filename to save the confusion matrix in\n",
    "        a PNG image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    # Compute and print the accuracy and AUC over train\n",
    "    train_acc = model.evaluate(\n",
    "        x=train_matrix, \n",
    "        y=np.array(train_labels))\n",
    "\n",
    "    print(f\"Accuracy over train dataset: {train_acc[1]}\")\n",
    "    print(f\"AUC over train dataset: {train_acc[2]}\\n\")\n",
    "\n",
    "    # Compute and print the accuracy and AUC over test\n",
    "    test_acc = model.evaluate(\n",
    "        x=test_matrix, \n",
    "        y=np.array(test_labels))\n",
    "\n",
    "    print(f\"Accuracy over test dataset: {test_acc[1]}\")\n",
    "    print(f\"AUC over test dataset: {test_acc[2]}\")\n",
    "\n",
    "    # Generate class label predictions over the test dataset\n",
    "    # Class 0 ~ <= 0.5 | Class 1 ~ > 0.5\n",
    "    test_preds = (model.predict(test_matrix) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Plot the confusion matrix \n",
    "    ConfusionMatrixDisplay(\n",
    "        confusion_matrix=confusion_matrix(\n",
    "            np.array(test_labels), \n",
    "            np.array(test_preds)), \n",
    "        display_labels=[\"Non-sexist\", \"Sexist\"]) \\\n",
    "    .plot()    \n",
    "\n",
    "    # Save the confusion matrix in an image\n",
    "    if (type(conf_matrix_filename) == str and\n",
    "        len(conf_matrix_filename) > 0):\n",
    "        plt.savefig(conf_matrix_filename)\n",
    "\n",
    "    # Save the metrics in a text file\n",
    "    if (type(metrics_filename) == str and\n",
    "        len(metrics_filename) > 0):\n",
    "        opened_file = open(metrics_filename, \"w\")\n",
    "        print(f\"Accuracy over train dataset: {train_acc[1]}\", file=opened_file) \n",
    "        print(f\"AUC over train dataset: {train_acc[2]}\\n\", file=opened_file) \n",
    "        print(f\"Accuracy over test dataset: {test_acc[1]}\", file=opened_file) \n",
    "        print(f\"AUC over test dataset: {test_acc[2]}\", file=opened_file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Mejor experimento: arquitectura LSTM\n",
    "\n",
    "Tras diversos experimentos con diferentes arquitecturas, embeddings, configuraciones de hiperparámetros y técnicas de procesamiento y aumento de textos, a continuación se presenta el modelo que mejores resultados ha proporcionado en relación a las métricas consideradas de *accuracy* y AUC. \n",
    "\n",
    "* **Procesamiento de los textos de entrenamiento y validación**.\n",
    "  * **Considera textos tanto en inglés como en español**.\n",
    "  * Elimina usuarios mencionados.\n",
    "  * Elimina caracteres no alfabéticos.\n",
    "  * Elimina *stopwords* en inglés y español.\n",
    "  * Elimina palabras sin vocales.\n",
    "  * <p>Convierte todos los caracteres a minúsculas.</p>\n",
    "\n",
    "* **Codificación de textos mediante embeddings**. Tras experimentar con diferentes ficheros de **Glove embeddings** encontrados en este [link](https://nlp.stanford.edu/projects/glove/), destacando que la gran mayoría únicamente disponen de términos en inglés, se ha empleado el fichero denominado **glove.twitter.27B.100d.txt** por **multilingüismo** y mayor capacidad de representación por el mayor número de términos que contiene. \n",
    "\n",
    "* **Arquitectura e hiperparámetros**.\n",
    "  * 1 capa de entrada para proporcionar los documentos procesados.\n",
    "  * **1 capa oculta con 128 neuronas**.\n",
    "  * 1 capa de salida con la que asignar una clase a cada muestra.\n",
    "  * **Tamaño del lote: 32**.\n",
    "  * <p>Máximo número de palabras que se mantienen en memoria: 1.000.</p>\n",
    "\n",
    "* **Entrenamiento y validación**.\n",
    "  * Número máximo de **iteraciones**: 100.\n",
    "  * **Early Stopping** tras 15 iteraciones sin mejorar el valor de la métrica *AUC* en validación y recuperando los pesos del mejor modelo encontrado. \n",
    "  * Porcentaje de **validación**: 20%.\n",
    "  * Función de pérdida: *binary_crossentropy*.\n",
    "  * Optimizador: Adam.\n",
    "  * **Métricas de validación**: *accuracy* y AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix:\n",
      "[[  0   0   0 ... 211 776  93]\n",
      " [  0   0   0 ... 142 455 856]\n",
      " [  0   0   0 ... 256 257 186]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...  53 327 518]\n",
      " [  0   0   0 ...   2 742   8]]\n",
      "Train labels:[1, 0, 1, 0, 0]\n",
      "\n",
      "Test matrix:\n",
      "[[  0   0   0 ... 259 299  32]\n",
      " [  0   0   0 ... 109   3  48]\n",
      " [  0   0   0 ... 974 180 974]\n",
      " ...\n",
      " [  0   0   0 ... 512 420  97]\n",
      " [  0   0   0 ...   8  37 462]\n",
      " [  0   0   0 ...   5 102 167]]\n",
      "Test labels:[0, 0, 1, 1, 0]\n",
      "\n",
      "Glove Twitter 27B 100d embedding matrix:\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.18897    -1.25849998 -0.32616001 ... -0.04941     0.32721001\n",
      "   0.24265   ]\n",
      " [-0.12749    -0.62423998  0.26901001 ...  0.56253999  0.62739998\n",
      "   0.23934001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.48534     0.38064    -1.05149996 ... -0.12978999 -0.26934001\n",
      "   0.57903999]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "MAX_N_WORDS = 1000\n",
    "SEQUENCE_MAX_LEN = 100\n",
    "EMBEDDING_FILE_PATH = \"../en_es_embeddings/glove.twitter.27B.100d.txt\"\n",
    "APPLY_LEMMATIZATION = False \n",
    "APPLY_STEMMING = False \n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 100\n",
    "VALID_RATE = 0.2\n",
    "MODEL_CALLBACKS = [EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    min_delta=0.001,\n",
    "    patience=15,\n",
    "    restore_best_weights=True)]\n",
    "LOSS_FUNCTION = \"binary_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "VALID_METRICS = [\"accuracy\", \"AUC\"]\n",
    "\n",
    "# Process the train and test documents as well as create\n",
    "# a tokenizer based on the processed train documents\n",
    "processed_data = get_train_test_matrix(\n",
    "    max_n_words=MAX_N_WORDS,\n",
    "    sequence_len=SEQUENCE_MAX_LEN,\n",
    "    lemm=False,\n",
    "    stemm=False\n",
    ")\n",
    "print(f\"Train matrix:\\n{processed_data['train_matrix']}\")\n",
    "print(f\"Train labels:{processed_data['train_labels'][0:5]}\")\n",
    "\n",
    "print(f\"\\nTest matrix:\\n{processed_data['test_matrix']}\")\n",
    "print(f\"Test labels:{processed_data['test_labels'][0:5]}\")\n",
    "\n",
    "# Load the embeddings stored in the defined file path\n",
    "# Encode the train matrix with these embeddings\n",
    "embedding_matrix = get_embedding_matrix(\n",
    "    embedding_file=EMBEDDING_FILE_PATH,\n",
    "    tokenizer=processed_data[\"tokenizer\"],\n",
    "    sequence_len=SEQUENCE_MAX_LEN\n",
    ")\n",
    "print(f\"\\nGlove Twitter 27B 100d embedding matrix:\\n{embedding_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 19:37:53.174398: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 100)          2515000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,632,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 2,515,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "175/175 [==============================] - 12s 58ms/step - loss: 0.6144 - accuracy: 0.6685 - auc: 0.7157 - val_loss: 0.7944 - val_accuracy: 0.4885 - val_auc: 0.6097\n",
      "Epoch 2/100\n",
      "175/175 [==============================] - 11s 61ms/step - loss: 0.5571 - accuracy: 0.7259 - auc: 0.7846 - val_loss: 0.6834 - val_accuracy: 0.6232 - val_auc: 0.6414\n",
      "Epoch 3/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.5265 - accuracy: 0.7438 - auc: 0.8118 - val_loss: 0.7729 - val_accuracy: 0.5974 - val_auc: 0.6483\n",
      "Epoch 4/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.5858 - accuracy: 0.7182 - auc: 0.7705 - val_loss: 0.7840 - val_accuracy: 0.5516 - val_auc: 0.6399\n",
      "Epoch 5/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.5081 - accuracy: 0.7551 - auc: 0.8267 - val_loss: 0.8877 - val_accuracy: 0.5322 - val_auc: 0.6409\n",
      "Epoch 6/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.4880 - accuracy: 0.7699 - auc: 0.8419 - val_loss: 0.7362 - val_accuracy: 0.6182 - val_auc: 0.6559\n",
      "Epoch 7/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.4709 - accuracy: 0.7859 - auc: 0.8549 - val_loss: 0.8314 - val_accuracy: 0.5917 - val_auc: 0.6506\n",
      "Epoch 8/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.4532 - accuracy: 0.7898 - auc: 0.8665 - val_loss: 0.7934 - val_accuracy: 0.6039 - val_auc: 0.6622\n",
      "Epoch 9/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.4360 - accuracy: 0.7991 - auc: 0.8767 - val_loss: 0.7648 - val_accuracy: 0.6332 - val_auc: 0.6620\n",
      "Epoch 10/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.4126 - accuracy: 0.8104 - auc: 0.8909 - val_loss: 0.9343 - val_accuracy: 0.5559 - val_auc: 0.6532\n",
      "Epoch 11/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.3912 - accuracy: 0.8260 - auc: 0.9027 - val_loss: 0.8513 - val_accuracy: 0.6117 - val_auc: 0.6593\n",
      "Epoch 12/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.3676 - accuracy: 0.8348 - auc: 0.9157 - val_loss: 0.9201 - val_accuracy: 0.5996 - val_auc: 0.6536\n",
      "Epoch 13/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.3386 - accuracy: 0.8516 - auc: 0.9297 - val_loss: 0.9661 - val_accuracy: 0.6010 - val_auc: 0.6459\n",
      "Epoch 14/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.3169 - accuracy: 0.8629 - auc: 0.9388 - val_loss: 0.9435 - val_accuracy: 0.6117 - val_auc: 0.6414\n",
      "Epoch 15/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.2916 - accuracy: 0.8756 - auc: 0.9491 - val_loss: 1.1566 - val_accuracy: 0.5723 - val_auc: 0.6485\n",
      "Epoch 16/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.2594 - accuracy: 0.8894 - auc: 0.9606 - val_loss: 1.4017 - val_accuracy: 0.5423 - val_auc: 0.6234\n",
      "Epoch 17/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.2378 - accuracy: 0.9043 - auc: 0.9676 - val_loss: 1.2528 - val_accuracy: 0.5709 - val_auc: 0.6404\n",
      "Epoch 18/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.2098 - accuracy: 0.9174 - auc: 0.9755 - val_loss: 1.3219 - val_accuracy: 0.5960 - val_auc: 0.6400\n",
      "Epoch 19/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.1752 - accuracy: 0.9314 - auc: 0.9836 - val_loss: 1.2662 - val_accuracy: 0.6246 - val_auc: 0.6478\n",
      "Epoch 20/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.1581 - accuracy: 0.9425 - auc: 0.9869 - val_loss: 1.3778 - val_accuracy: 0.6074 - val_auc: 0.6413\n",
      "Epoch 21/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.1377 - accuracy: 0.9502 - auc: 0.9902 - val_loss: 1.5964 - val_accuracy: 0.5723 - val_auc: 0.6214\n",
      "Epoch 22/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.1167 - accuracy: 0.9606 - auc: 0.9931 - val_loss: 1.5724 - val_accuracy: 0.5981 - val_auc: 0.6318\n",
      "Epoch 23/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.1093 - accuracy: 0.9609 - auc: 0.9943 - val_loss: 1.6493 - val_accuracy: 0.6125 - val_auc: 0.6347\n",
      "Epoch 24/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0992 - accuracy: 0.9633 - auc: 0.9951 - val_loss: 1.8182 - val_accuracy: 0.5716 - val_auc: 0.6326\n",
      "Epoch 25/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0815 - accuracy: 0.9706 - auc: 0.9968 - val_loss: 1.8548 - val_accuracy: 0.5788 - val_auc: 0.6320\n",
      "Epoch 26/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0891 - accuracy: 0.9676 - auc: 0.9952 - val_loss: 2.0991 - val_accuracy: 0.5473 - val_auc: 0.6246\n",
      "Epoch 27/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0941 - accuracy: 0.9658 - auc: 0.9957 - val_loss: 2.1005 - val_accuracy: 0.5516 - val_auc: 0.6277\n",
      "Epoch 28/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0737 - accuracy: 0.9742 - auc: 0.9974 - val_loss: 1.8441 - val_accuracy: 0.5766 - val_auc: 0.6332\n",
      "Epoch 29/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0623 - accuracy: 0.9763 - auc: 0.9981 - val_loss: 1.8111 - val_accuracy: 0.5946 - val_auc: 0.6323\n",
      "Epoch 30/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.0564 - accuracy: 0.9789 - auc: 0.9984 - val_loss: 2.0049 - val_accuracy: 0.5845 - val_auc: 0.6327\n",
      "Epoch 31/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0530 - accuracy: 0.9805 - auc: 0.9985 - val_loss: 2.1095 - val_accuracy: 0.5924 - val_auc: 0.6338\n",
      "Epoch 32/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0512 - accuracy: 0.9792 - auc: 0.9985 - val_loss: 2.2002 - val_accuracy: 0.5845 - val_auc: 0.6358\n",
      "Epoch 33/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0497 - accuracy: 0.9814 - auc: 0.9986 - val_loss: 2.2351 - val_accuracy: 0.5938 - val_auc: 0.6298\n",
      "Epoch 34/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.1098 - accuracy: 0.9622 - auc: 0.9923 - val_loss: 1.9973 - val_accuracy: 0.5745 - val_auc: 0.6330\n",
      "Epoch 35/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.0551 - accuracy: 0.9794 - auc: 0.9985 - val_loss: 2.1198 - val_accuracy: 0.5652 - val_auc: 0.6305\n",
      "Epoch 36/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.0472 - accuracy: 0.9821 - auc: 0.9988 - val_loss: 2.5081 - val_accuracy: 0.5781 - val_auc: 0.6296\n",
      "Epoch 37/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.0460 - accuracy: 0.9821 - auc: 0.9988 - val_loss: 2.0890 - val_accuracy: 0.5895 - val_auc: 0.6362\n",
      "Epoch 38/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0434 - accuracy: 0.9823 - auc: 0.9988 - val_loss: 2.2338 - val_accuracy: 0.5917 - val_auc: 0.6367\n",
      "Epoch 39/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0419 - accuracy: 0.9830 - auc: 0.9989 - val_loss: 2.4173 - val_accuracy: 0.5781 - val_auc: 0.6318\n",
      "Epoch 40/100\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 0.0404 - accuracy: 0.9835 - auc: 0.9990 - val_loss: 2.4195 - val_accuracy: 0.5867 - val_auc: 0.6384\n",
      "Epoch 41/100\n",
      "175/175 [==============================] - 13s 74ms/step - loss: 0.0417 - accuracy: 0.9814 - auc: 0.9989 - val_loss: 2.3413 - val_accuracy: 0.5946 - val_auc: 0.6332\n",
      "Epoch 42/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.0414 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.5762 - val_accuracy: 0.5867 - val_auc: 0.6309\n",
      "Epoch 43/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0443 - accuracy: 0.9808 - auc: 0.9987 - val_loss: 2.3822 - val_accuracy: 0.5860 - val_auc: 0.6274\n",
      "Epoch 44/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0936 - accuracy: 0.9645 - auc: 0.9936 - val_loss: 2.2264 - val_accuracy: 0.6067 - val_auc: 0.6421\n",
      "Epoch 45/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0559 - accuracy: 0.9778 - auc: 0.9982 - val_loss: 2.2449 - val_accuracy: 0.5974 - val_auc: 0.6450\n",
      "Epoch 46/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0415 - accuracy: 0.9824 - auc: 0.9990 - val_loss: 2.4859 - val_accuracy: 0.5860 - val_auc: 0.6398\n",
      "Epoch 47/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0387 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.4237 - val_accuracy: 0.5881 - val_auc: 0.6382\n",
      "Epoch 48/100\n",
      "175/175 [==============================] - 11s 61ms/step - loss: 0.0384 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.5644 - val_accuracy: 0.5874 - val_auc: 0.6367\n",
      "Epoch 49/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0376 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.3943 - val_accuracy: 0.6003 - val_auc: 0.6424\n",
      "Epoch 50/100\n",
      "175/175 [==============================] - 11s 61ms/step - loss: 0.0387 - accuracy: 0.9812 - auc: 0.9989 - val_loss: 2.4670 - val_accuracy: 0.5967 - val_auc: 0.6389\n",
      "Epoch 51/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0383 - accuracy: 0.9824 - auc: 0.9990 - val_loss: 2.5569 - val_accuracy: 0.6003 - val_auc: 0.6398\n",
      "Epoch 52/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0389 - accuracy: 0.9810 - auc: 0.9990 - val_loss: 2.5992 - val_accuracy: 0.6024 - val_auc: 0.6393\n",
      "Epoch 53/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0383 - accuracy: 0.9824 - auc: 0.9989 - val_loss: 2.4766 - val_accuracy: 0.6046 - val_auc: 0.6382\n",
      "Epoch 54/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0390 - accuracy: 0.9810 - auc: 0.9989 - val_loss: 2.5231 - val_accuracy: 0.5953 - val_auc: 0.6358\n",
      "Epoch 55/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0374 - accuracy: 0.9837 - auc: 0.9990 - val_loss: 2.6033 - val_accuracy: 0.5931 - val_auc: 0.6349\n",
      "Epoch 56/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0374 - accuracy: 0.9826 - auc: 0.9990 - val_loss: 2.5075 - val_accuracy: 0.5996 - val_auc: 0.6389\n",
      "Epoch 57/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0363 - accuracy: 0.9821 - auc: 0.9991 - val_loss: 2.6056 - val_accuracy: 0.5845 - val_auc: 0.6312\n",
      "Epoch 58/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0381 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.4343 - val_accuracy: 0.6196 - val_auc: 0.6418\n",
      "Epoch 59/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.0374 - accuracy: 0.9815 - auc: 0.9990 - val_loss: 2.7827 - val_accuracy: 0.5838 - val_auc: 0.6321\n",
      "Epoch 60/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.0360 - accuracy: 0.9817 - auc: 0.9990 - val_loss: 2.7781 - val_accuracy: 0.5917 - val_auc: 0.6347\n",
      "Epoch 61/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.0369 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.7190 - val_accuracy: 0.5831 - val_auc: 0.6333\n",
      "Epoch 62/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0370 - accuracy: 0.9824 - auc: 0.9990 - val_loss: 2.7322 - val_accuracy: 0.6046 - val_auc: 0.6358\n",
      "Epoch 63/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0374 - accuracy: 0.9814 - auc: 0.9990 - val_loss: 2.8087 - val_accuracy: 0.6003 - val_auc: 0.6370\n",
      "Epoch 64/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0849 - accuracy: 0.9645 - auc: 0.9947 - val_loss: 2.5729 - val_accuracy: 0.5344 - val_auc: 0.6073\n",
      "Epoch 65/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.1069 - accuracy: 0.9586 - auc: 0.9926 - val_loss: 2.4408 - val_accuracy: 0.5595 - val_auc: 0.6302\n",
      "Epoch 66/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.0511 - accuracy: 0.9787 - auc: 0.9984 - val_loss: 2.2360 - val_accuracy: 0.5759 - val_auc: 0.6342\n",
      "Epoch 67/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0376 - accuracy: 0.9842 - auc: 0.9990 - val_loss: 2.3718 - val_accuracy: 0.5809 - val_auc: 0.6365\n",
      "Epoch 68/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0358 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.3910 - val_accuracy: 0.5881 - val_auc: 0.6388\n",
      "Epoch 69/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.0344 - accuracy: 0.9832 - auc: 0.9991 - val_loss: 2.4734 - val_accuracy: 0.5838 - val_auc: 0.6338\n",
      "Epoch 70/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0343 - accuracy: 0.9830 - auc: 0.9991 - val_loss: 2.3862 - val_accuracy: 0.6017 - val_auc: 0.6364\n",
      "Epoch 71/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0345 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.5084 - val_accuracy: 0.5953 - val_auc: 0.6367\n",
      "Epoch 72/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0346 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.6472 - val_accuracy: 0.5867 - val_auc: 0.6356\n",
      "Epoch 73/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0342 - accuracy: 0.9819 - auc: 0.9991 - val_loss: 2.6556 - val_accuracy: 0.5817 - val_auc: 0.6334\n",
      "Epoch 74/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.0347 - accuracy: 0.9828 - auc: 0.9990 - val_loss: 2.5583 - val_accuracy: 0.5881 - val_auc: 0.6345\n",
      "Epoch 75/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0343 - accuracy: 0.9815 - auc: 0.9991 - val_loss: 2.7177 - val_accuracy: 0.5817 - val_auc: 0.6332\n",
      "Epoch 76/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.0354 - accuracy: 0.9814 - auc: 0.9991 - val_loss: 2.6292 - val_accuracy: 0.5903 - val_auc: 0.6343\n",
      "Epoch 77/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.0341 - accuracy: 0.9821 - auc: 0.9991 - val_loss: 2.7530 - val_accuracy: 0.5903 - val_auc: 0.6337\n",
      "Epoch 78/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0344 - accuracy: 0.9826 - auc: 0.9990 - val_loss: 2.6326 - val_accuracy: 0.5881 - val_auc: 0.6408\n",
      "Epoch 79/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0346 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.7244 - val_accuracy: 0.5888 - val_auc: 0.6349\n",
      "Epoch 80/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.0356 - accuracy: 0.9815 - auc: 0.9990 - val_loss: 2.7784 - val_accuracy: 0.5802 - val_auc: 0.6359\n",
      "Epoch 81/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0356 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.6132 - val_accuracy: 0.5924 - val_auc: 0.6348\n",
      "Epoch 82/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.0343 - accuracy: 0.9821 - auc: 0.9991 - val_loss: 2.8582 - val_accuracy: 0.5838 - val_auc: 0.6343\n",
      "Epoch 83/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0343 - accuracy: 0.9828 - auc: 0.9991 - val_loss: 2.8258 - val_accuracy: 0.5838 - val_auc: 0.6319\n",
      "Epoch 84/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.0346 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.6188 - val_accuracy: 0.6010 - val_auc: 0.6365\n",
      "Epoch 85/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0347 - accuracy: 0.9828 - auc: 0.9990 - val_loss: 2.8596 - val_accuracy: 0.5895 - val_auc: 0.6325\n",
      "Epoch 86/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.0347 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.7885 - val_accuracy: 0.5981 - val_auc: 0.6377\n",
      "Epoch 87/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0350 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.7070 - val_accuracy: 0.5953 - val_auc: 0.6394\n",
      "Epoch 88/100\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 0.0353 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.9671 - val_accuracy: 0.5888 - val_auc: 0.6347\n",
      "Epoch 89/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.0351 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.7911 - val_accuracy: 0.5996 - val_auc: 0.6373\n",
      "Epoch 90/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0357 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.5553 - val_accuracy: 0.5989 - val_auc: 0.6286\n",
      "Epoch 91/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.1129 - accuracy: 0.9566 - auc: 0.9912 - val_loss: 2.2288 - val_accuracy: 0.5960 - val_auc: 0.6391\n",
      "Epoch 92/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0520 - accuracy: 0.9785 - auc: 0.9979 - val_loss: 2.0911 - val_accuracy: 0.6032 - val_auc: 0.6434\n",
      "Epoch 93/100\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 0.0361 - accuracy: 0.9819 - auc: 0.9991 - val_loss: 2.3197 - val_accuracy: 0.5881 - val_auc: 0.6393\n",
      "Epoch 94/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0335 - accuracy: 0.9826 - auc: 0.9991 - val_loss: 2.3958 - val_accuracy: 0.5924 - val_auc: 0.6389\n",
      "Epoch 95/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.0332 - accuracy: 0.9835 - auc: 0.9990 - val_loss: 2.4266 - val_accuracy: 0.5910 - val_auc: 0.6401\n",
      "Epoch 96/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0332 - accuracy: 0.9817 - auc: 0.9991 - val_loss: 2.4551 - val_accuracy: 0.5946 - val_auc: 0.6390\n",
      "Epoch 97/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.0335 - accuracy: 0.9830 - auc: 0.9990 - val_loss: 2.5749 - val_accuracy: 0.5903 - val_auc: 0.6347\n",
      "Epoch 98/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0332 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.6152 - val_accuracy: 0.5860 - val_auc: 0.6356\n",
      "Epoch 99/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.0326 - accuracy: 0.9826 - auc: 0.9991 - val_loss: 2.5622 - val_accuracy: 0.5967 - val_auc: 0.6396\n",
      "Epoch 100/100\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 0.0329 - accuracy: 0.9826 - auc: 0.9992 - val_loss: 2.5926 - val_accuracy: 0.5888 - val_auc: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: ../outputs/lstm_model/assets\n"
     ]
    }
   ],
   "source": [
    "## Input layer\n",
    "input_layer = Input(\n",
    "    name=\"inputs\",\n",
    "    shape=[SEQUENCE_MAX_LEN])\n",
    "\n",
    "## Embedding layer: pre-trained embeddings\n",
    "layer = Embedding(\n",
    "    input_dim=len(processed_data[\"tokenizer\"].word_index)+1,\n",
    "    output_dim=SEQUENCE_MAX_LEN,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_N_WORDS,\n",
    "    trainable=False)(input_layer)\n",
    "\n",
    "## LSTM layer\n",
    "layer = LSTM(units=128)(layer)\n",
    "\n",
    "## Output layer\n",
    "layer = Dense(\n",
    "    name=\"output\",\n",
    "    units=1)(layer)\n",
    "\n",
    "## Activation layer\n",
    "output_layer = Activation(activation=\"sigmoid\")(layer)\n",
    "\n",
    "## Model object\n",
    "lstm_model = Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=output_layer)\n",
    "\n",
    "## Summary of the model\n",
    "lstm_model.summary()\n",
    "\n",
    "## Compile the model \n",
    "lstm_model.compile(\n",
    "    loss=LOSS_FUNCTION,\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=VALID_METRICS)\n",
    "\n",
    "## Train the built model\n",
    "lstm_model.fit(\n",
    "    x=processed_data[\"train_matrix\"], \n",
    "    y=np.array(processed_data[\"train_labels\"]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=N_EPOCHS,\n",
    "    validation_split=VALID_RATE)\n",
    "\n",
    "# Save the entire model in a folder\n",
    "lstm_model.save('../outputs/lstm_model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se puede apreciar en los siguientes resultados, la tasa de aciertos sobre el conjunto de entrenamiento es bastante alta con un 90% aunque sobre el conjunto de **test apenas alcanza el 67% de accuracy**. Considerando el ínfimo valor del área bajo la curva ROC en base a este conjunto de datos se puede concluir que la **capacidad de predicción del modelo no es aceptable**, siendo apenas superior a la que tendría un clasificador aleatorio. \n",
    "\n",
    "Observando la matriz de confusión es altamente notable la **elevada tasa de falsos negativos**, es decir, textos sexistas que no han sido detectados. Por lo tanto una arquitectura LSTM y la mejor configuración encontrada en este grupo de experimentos, parece no ser suficiente precisa como para construir un clasificador de calidad capaz de identificar textos sexistas y no sexistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 4s 20ms/step - loss: 0.5430 - accuracy: 0.9051 - auc: 0.9456\n",
      "Accuracy over train dataset: 0.9051167964935303\n",
      "AUC over train dataset: 0.9455955028533936\n",
      "\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 2.0292 - accuracy: 0.6658 - auc: 0.7131\n",
      "Accuracy over test dataset: 0.6657509207725525\n",
      "AUC over test dataset: 0.713101863861084\n",
      "137/137 [==============================] - 3s 20ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Evaluate the trained LSTM model over train and test datasets\n",
    "validate_lstm_model(\n",
    "    model=lstm_model,\n",
    "    train_matrix=processed_data[\"train_matrix\"],\n",
    "    train_labels=processed_data[\"train_labels\"],\n",
    "    test_matrix=processed_data[\"test_matrix\"],\n",
    "    test_labels=processed_data[\"test_labels\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de resultados\n",
    "\n",
    "Con el propósito de tratar de identificar la existencia de posibles patrones que dificultan la construcción de un clasificador de calidad para detectar textos sexistas y no sexistas, se ha realizado la siguiente experimentación. Consiste en evaluar las **muestras mal clasificadas** de cada categoría a partir de intervalos de confianza para posteriormente cruzar los ejemplos resultantes con el **análisis de emociones y las categorías sexistas** de la columna *task2*, para las muestras de la clase positiva. \n",
    "\n",
    "##### Análisis de falsos negativos\n",
    "\n",
    "Se trata de muestras positivas erróneamente clasificadas como textos no sexistas que **suponen más del 65% de errores** cometidos por el modelo anterior. Tal y como podemos observar, los ejemplos positivos clasificados como negativos se caracterizan por haber sido categorizados bajo una **confianza ínfima**, siendo la máxima encontrada de un 42,7%. \n",
    "\n",
    "| Intervalo de confianza | Nº muestras |\n",
    "| --- | --- |\n",
    "| >= 90% | 0 textos |\n",
    "| >= 60% | 0 textos |\n",
    "| < 60% | 955 textos |\n",
    "\n",
    "Acompañando este análisis con las categorías sexistas disponibles en la segunda variable dependiente, podemos apreciar en los siguientes resultados que las tres primeras etiquetas se corresponden con las **clases más difíciles de predecir**. Recordando el análisis exploratorio realizado en el notebook *eda.ipynb*, varias fuentes consultadas acerca de la naturaleza de los datos y los resultados de las dos competiciones realizadas, una de las denuncias más comunes era la **dificultad entrañada en detectar textos sexistas cuyo contenido no se caracterizase mayormente por disponer de términos violentos y negativos**.\n",
    "\n",
    "* *'ideological-inequality'*: 241 textos\n",
    "* *'misogyny-non-sexual-violence'*: 210 textos\n",
    "* *'stereotyping-dominance'*: 193 textos\n",
    "* *'sexual-violence'*: 178 textos\n",
    "\n",
    "Si además consideramos los textos clasificados según diversas emociones, tal y como se puede comprobar a continuación la **segunda emoción más poblada es *joy***, caracterizada por documentos con **terminología positiva** aunque con **significados tremendamente negativos** desde un **punto de vista ideológico**. Tras una inspección visual de los documentos categorizados dentro de las otras dos emociones he podido concluir que la gran mayoría de ellos contienen un **alto porcentaje de ironía y diversidad de temáticas** cuyo vocabulario alude más a otros tópicos como la iglesia, la lucha LGTBI, la política, el racismo, etc.\n",
    "\n",
    "* 'anger': 340 textos\n",
    "* 'joy': 313 textos\n",
    "* 'fear': 223 textos\n",
    "\n",
    "##### Análisis de falsos positivos\n",
    "\n",
    "El caso contrario consiste en analizar aquellos documentos negativos clasificados erróneamente como textos sexistas que suponen una minoría de errores cometidos por el modelo. Al contrario que en la situación anterior, la **gran mayoría de muestras mal clasificadas se encuentran en intervalos de confianza razonablemente altos** por lo que el modelo confía más en las predicciones de los falsos positivos que de los negativos.\n",
    "\n",
    "| Intervalo de confianza | Nº muestras |\n",
    "| --- | --- |\n",
    "| >= 90% | 372 textos |\n",
    "| >= 60% | 486 textos |\n",
    "| < 60% | 19 textos |\n",
    "\n",
    "Si finalmente cruzamos estos resultados con las emociones detectadas se ha podido descubrir que la **amplia mayoría de documentos se encuentran dentro de *joy***, una categoría bastante complicada de aprender según se ha podido comprobar hasta el momento. A continuación acompañan en el ranking las emociones ***anger* y *fear***, tal y como se mostraba en el análisis de falsos negativos. Después de inspeccionar visualmente los documentos pertenecientes a las tres emociones he podido detectar una característica común: el uso de **términos negativos y sexistas en reivindicaciones** contrarias a la ideología machista. Por lo tanto se trata de la misma situación que en el caso anterior, las muestras mal clasificadas contienen **vocabulario sexista y violento con un significado positivo pero más complejo de detectar**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Mejor experimento: arquitectura BiLSTM\n",
    "\n",
    "Apoyado en los experimentos y resultados con una arquitectura LSTM se ha podido descubrir que prácticamente la totalidad de las **configuraciones y conclusiones anteriores son perfectamente aplicables a un clasificador BiLSTM**. La única **excepción** existente es que en este caso resulta más ventajoso añadir **una capa oculta más con 128 neuronas** para aumentar las métricas de *accuracy* y AUC en aproximadamente un 2%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones de experimentos con LSTM y BiLSTM\n",
    "\n",
    "### 6.1. Técnicas de procesamiento de textos\n",
    "\n",
    "  * **Ni la lematización ni el stemming han conseguido apenas mejorar** la capacidad predictiva de los modelos aunque sí que aumenta el tiempo y recursos de computación, especialmente con la primera técnica. Por lo tanto en esta arquitectura de ejemplo no se aplican ninguno de los dos métodos.\n",
    "\n",
    "  * Otra técnica empleada ha sido la **detección y corrección de palabras mal escritas** puesto que las fuentes de datos de las que proceden los documentos son redes sociales y es bien conocido que en estos medios la escritura de textos no es ni precisa ni correcta en la mayoría de ocasiones. Si bien el objetivo era **aumentar el número de palabras codificables** por los *embeddings* incrementando la representatividad de las matrices de entrada a los modelos, **tampoco se ha conseguido incrementar la capacidad de predicción** de los modelos. Analizando las posibles teorías explicativas de este suceso, se han calculado las siguientes métricas para el conjunto de entrenamiento:\n",
    "\n",
    "    * Número de palabras totales: 201.361\n",
    "    * Número de términos incorrectos detectados: 19.008\n",
    "    * Número de términos incorrectos corregidos: 15.341\n",
    "\n",
    "    Tal y como se puede apreciar **únicamente existe un 10% aproximadamente de términos incorrectos** en los textos de entrenamiento aunque se han corregido más de un 80% de ellos. Dadas estas cifras, parece razonable el hecho de no notar mejoría alguna aplicando esta técnica para el entrenamiento de modelos LSTM.\n",
    "\n",
    "### 6.2. Embeddings\n",
    "\n",
    "Tras codificar los documentos de entrenamiento y validación con varios de los ficheros de **embeddings** relativos a este [link](https://nlp.stanford.edu/projects/glove/) utilizando la arquitectura explicada anteriormente, se ha podido determinar que aquellos **embeddings basados en un vocabulario más voluminoso** proporcionan mejores métricas de *accuracy* y AUC por aumentar la representatividad de los textos al **codificar un mayor número de palabras**. \n",
    "\n",
    "No obstante no ocurre lo mismo con el **número de vectores**, puesto que los ficheros de 200 y 300 disparan el tiempo de entrenamiento y validación mientras que los valores de las anteriores métricas se mantienen prácticamente invariables con respecto al uso del fichero con únicamente 100 vectores. \n",
    "\n",
    "### 6.3. Batch size\n",
    "\n",
    "En este apartado se destaca la experimentación con diferentes tamaños de lote, siendo los valores más comunes: **128, 64 y 32**. Según los resultados obtenidos **apenas existe diferencia entre los dos primeros** en relación a las métricas de *accuracy* y AUC, invirtiendo un menor número de recursos computacionales en el primer caso con una cifra mayor. Ha sido el **tamaño de lote 32** el que ha conseguido **mejorar las métricas de validación en más de un 2%** a costa del incremento del tiempo de entrenamiento y validación en menos de 5 minutos, por lo que es el que ha sido seleccionado para continuar con la experimentación.\n",
    "\n",
    "### 6.5. Configuración de entrenamiento\n",
    "\n",
    "La inclusión de la técnica **Early Stopping** ha sido **crucial para evitar el *overfitting*** y el malgasto de recursos y tiempo computacional para obtener un modelo con peor calidad a costa de un mayor número de iteraciones. Para su configuración se han probado diferentes métricas como *loss*, *accuracy* y ***AUC***, siendo esta última la elegida por demostrar un **equilibrio entre el tiempo invertido y su capacidad predictiva**, ya que con *accuracy* se conseguían clasificadores muy tempranos recortando considerablemente el número de iteraciones utilizadas para su construcción. \n",
    "\n",
    "### 6.6. Data Augmentation\n",
    "\n",
    "* **Traducción de textos**. Esta primera técnica se ha empleado para aumentar el número de muestras del conjunto de entrenamiento traduciendo los **textos ingleses a español y viceversa**, consiguiendo el doble de documentos. \n",
    "\n",
    "* **Easy Data Augmentation**. Conocida como EDA se trata de un conjunto de técnicas con las que se generan nuevos textos añadiendo **sinónimos** de palabras elegidas aleatoriamente, modificando la **posición** de términos de manera aleatoria e incluso **eliminando** conceptos con una cierta probabilidad aleatoria.\n",
    "\n",
    "* **Round-Trip Translation**. También denominada RTT es una metodología capaz de traducir documentos de su idioma origen a otro especificado para generar nuevos documentos a partir de una **segunda traducción a la inversa** aprovechando las variaciones incluidas en esta fase debido a la interpretación de los traductores. \n",
    "\n",
    "* **Contextual Word Embeddings**. Esta técnica emplea modelos de **parafraseo y/o transformers** para generar nuevos textos a partir del **contexto** de las muestras proporcionadas con el objetivo de mantener su significado y así proporcionar un conjunto de documentos más homogeneo.\n",
    "\n",
    "Si bien se han utilizado diferentes procesos con los que aumentar el conjunto de entrenamiento, no se ha podido descubrir ninguna mejora importante al construir modelos LSTM y BiLSTM.\n",
    "\n",
    "### 6.7. Arquitecturas\n",
    "\n",
    "Se han experimentado con diversas arquitecturas añadiendo/eliminando número de neuronas, capas ocultas, combinando capas bidireccionales con unidireccionales, añadiendo capas *drop out*, y a las conclusiones a las que se han podido alcanzar residen en que para el **conjunto de entrenamiento original**, es suficiente con un clasificador de **una capa oculta y 128 neuronas** ya que con arquitecturas más complejas no se consiguen aumentar las métricas de *accuracy* y AUC. Sin embargo, empleando **conjuntos de entrenamiento ampliados con documentos sintéticos**, sí se ha podido descubrir que es ligeramente beneficioso añadir un **mayor número de capas ocultas**. Así las dos arquitecturas que mejores resultados han proporcionado por detrás de la planteada en este notebook son:\n",
    "\n",
    "- Arquitectura de 2 capas ocultas con 128 neuronas cada una.\n",
    "- Arquitectura de 3 capas ocultas con 128, 64 y 32 neuronas, respectivamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00f7dc61815a6da5453fee0a1d7c3baaa88d552412e55cf65ecdf10d17265d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
