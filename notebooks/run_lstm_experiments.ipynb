{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimentación no determínistica con modelos LSTM y BiLSTM\n",
        "\n",
        "El objetivo de este notebook consiste en incluir el código necesario para **entrenar y validar múltiples modelos** con los que posteriormente conocer su calidad calculando la **media de valores de accuracy y AUC** comprobando así si las modificaciones realizadas aportan calidad a su capacidad predictiva.\n",
        "\n",
        "Como el propósito de este notebook consiste en ejecutarlo dentro de un software en la nube, como Google Colab, será necesario disponer de los **conjuntos de entrenamiento y validación en ficheros** para ahorrar tiempo de computación y recursos, centrandose únicamente en los modelos. Adicionalmente se deberá disponer de un fichero *requirements.txt* para **instalar las librerías** necesarias dentro del entorno cloud escogido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install libraries based on a requirements file\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Import required libraries for data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Import required libraries for building LSTM and BiLSTM models\n",
        "from keras.models import Model\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.layers import LSTM, Activation, Dense, Input, Embedding, Bidirectional\n",
        "\n",
        "# Import time library to measure the time spent on the experiments\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the datasets\n",
        "train_df = pd.read_csv(<path to the train dataset>)\n",
        "test_df = pd.read_csv(<path to the test dataset>)\n",
        "\n",
        "# Filter the datasets by language\n",
        "train_df = train_df[train_df['language'] == 'en'] # 'es'\n",
        "test_df = test_df[test_df['language'] == 'en'] # 'es'\n",
        "\n",
        "print(f'Number of train samples: {train_df.shape[0]}')\n",
        "print(f'Number of test samples: {test_df.shape[0]}')\n",
        "\n",
        "# Variables to store the validation metrics per each experiment\n",
        "train_accuracy_values = []\n",
        "test_accuracy_values = []\n",
        "train_auc_values = []\n",
        "test_auc_values = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ldf-z3gaEnh"
      },
      "outputs": [],
      "source": [
        "def get_train_test_matrix(\n",
        "    train_df: pd.DataFrame, test_df: pd.DataFrame, \n",
        "    max_n_words: int, sequence_len: int):\n",
        "    '''\n",
        "    Encodes the provided train and test datasets to convert\n",
        "    them into numeric vector sequences so they could be\n",
        "    encoded again using word embeddings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_df : Pandas dataframe\n",
        "        It contains the training data samples.\n",
        "    test_df: Pandas dataframe\n",
        "        It contains the testing data samples.\n",
        "    max_n_words : int\n",
        "        The number of words to store in memory.\n",
        "    sequence_len : int\n",
        "        The fixed size of the vector sequences.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    A dictionary with the created tokenizer, the train and \n",
        "    test numeric sequence vectors plus two lists with the\n",
        "    train and test class labels.\n",
        "    '''\n",
        "    # Create a tokenizer based on train texts\n",
        "    tokenizer = Tokenizer(num_words=max_n_words)\n",
        "    tokenizer.fit_on_texts(train_df['clean_text'].astype('str'))\n",
        "\n",
        "    # Transform each train text into a numeric sequence\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_df['clean_text'].astype('str'))\n",
        "\n",
        "    # Transform each train numeric sequence into a 2D vector\n",
        "    train_matrix = pad_sequences(\n",
        "        sequences=train_sequences, \n",
        "        maxlen=sequence_len)\n",
        "\n",
        "    # Tokenize the test documents using the trained tokenizer\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_df['clean_text'].astype('str'))\n",
        "\n",
        "    # Transform each test numeric sequence into a 2D vector\n",
        "    test_matrix = pad_sequences(\n",
        "        sequences=test_sequences,\n",
        "        maxlen=sequence_len)\n",
        "\n",
        "    return {\n",
        "        'tokenizer': tokenizer,\n",
        "        'train_matrix': train_matrix,\n",
        "        'train_labels': list(train_df['task1'].values),\n",
        "        'test_matrix': test_matrix,\n",
        "        'test_labels': list(test_df['task1'].values)\n",
        "    }\n",
        "\n",
        "\n",
        "def get_embedding_matrix(embedding_file: str, tokenizer: Tokenizer, sequence_len: int):\n",
        "    '''\n",
        "    Loads the embeddings of the provided file to then encode\n",
        "    the vocabulary stored in the provided tokenizer creating \n",
        "    the embedding vectors with the specified fixed size.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    embedding_file : str\n",
        "        The path to the file which contains the embeddings to use.\n",
        "    tokenizer : Keras Tokenizer\n",
        "        A Keras Tokenizer object that contains the vocabulary \n",
        "        from the train samples.\n",
        "    sequence_len : int\n",
        "        The fixed size of the embedding vector.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    A Numpy matrix with the encoded vocabulary as embeddings.\n",
        "    '''\n",
        "    # Load the embeddings stored in a TXT file\n",
        "    embedding_file = open(embedding_file)\n",
        "\n",
        "    # Store each word with its embeddings\n",
        "    embeddings_index = {\n",
        "        line.split()[0]:np.asarray(line.split()[1:], dtype='float32') \n",
        "        for line in embedding_file\n",
        "    }\n",
        "\n",
        "    # Initialize the embedding matrix with zeros\n",
        "    embedding_matrix = np.zeros(shape=(len(tokenizer.word_index)+1, sequence_len))\n",
        "\n",
        "    # Complete the matrix with the prior loaded embeddings\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        # Search for the embeddings of each word\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "        # Words not found will be zeros\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix    \n",
        "\n",
        "\n",
        "def validate_lstm_model(\n",
        "    model: Model, \n",
        "    train_matrix: np.ndarray, train_labels: list, \n",
        "    test_matrix: np.ndarray, test_labels: list):\n",
        "    '''\n",
        "    Computes the validation metrics of accuracy and AUC based\n",
        "    on the provided trained LSTM model over the train and test\n",
        "    data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : Keras model\n",
        "        A Keras LSTM trained model to validate.\n",
        "    train_matrix : Numpy matrix\n",
        "        A numeric sequence vectors that represent the encoded \n",
        "        train documents.\n",
        "    train_labels : list\n",
        "        A list of numbers with the encoded train class labels.\n",
        "    test_matrix : Numpy matrix\n",
        "        A numeric sequence vectors that represent the encoded \n",
        "        test documents.\n",
        "    test_labels : list\n",
        "        A list of numbers with the encoded test class labels.\n",
        "    '''\n",
        "    # Compute the train validation metrics\n",
        "    train_acc = model.evaluate(\n",
        "        x=train_matrix, \n",
        "        y=np.array(train_labels))\n",
        "    \n",
        "    train_accuracy_values.append(train_acc[1])\n",
        "    train_auc_values.append(train_acc[2])\n",
        "\n",
        "    # Compute the test validation metrics\n",
        "    test_acc = model.evaluate(\n",
        "        x=test_matrix, \n",
        "        y=np.array(test_labels))\n",
        "    \n",
        "    test_accuracy_values.append(test_acc[1])\n",
        "    test_auc_values.append(test_acc[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqCMTxtnaJKt",
        "outputId": "5c7c43a1-aed5-45f1-81ff-46cb6272665e"
      },
      "outputs": [],
      "source": [
        "# Experiment settings\n",
        "N_ITERATIONS = 30\n",
        "\n",
        "# Tokenizer and embedding settings\n",
        "MAX_N_WORDS = 1000\n",
        "EMBEDDING_FILE_PATH = <path to an embedding file>\n",
        "SEQUENCE_MAX_LEN = <int and depends on the number of vectors of the embedding file>\n",
        "\n",
        "# Training settings\n",
        "BATCH_SIZE = 16\n",
        "N_EPOCHS = 100\n",
        "VALID_RATE = 0.2\n",
        "MODEL_CALLBACKS = [EarlyStopping(\n",
        "    monitor='val_auc',\n",
        "    min_delta=0.001,\n",
        "    patience=15,\n",
        "    restore_best_weights=True)]\n",
        "LOSS_FUNCTION = 'binary_crossentropy'\n",
        "OPTIMIZER = 'adam'\n",
        "VALID_METRICS = ['accuracy', 'AUC']\n",
        "\n",
        "## START MEASURING TIME\n",
        "start = time.time()\n",
        "\n",
        "for i in range(0, N_ITERATIONS):\n",
        "    lstm_data = get_train_test_matrix(\n",
        "        train_df=train_df.sample(frac=1, axis=1),\n",
        "        test_df=test_df,\n",
        "        max_n_words=MAX_N_WORDS,\n",
        "        sequence_len=SEQUENCE_MAX_LEN\n",
        "    )\n",
        "\n",
        "    lstm_embedding_matrix = get_embedding_matrix(\n",
        "        embedding_file=EMBEDDING_FILE_PATH,\n",
        "        tokenizer=lstm_data['tokenizer'],\n",
        "        sequence_len=SEQUENCE_MAX_LEN\n",
        "    )\n",
        "\n",
        "    # Model ARCHITECTURE\n",
        "    input_layer = Input(\n",
        "        name='inputs',\n",
        "        shape=[SEQUENCE_MAX_LEN])\n",
        "\n",
        "    ## Embedding layer: pre-trained embeddings\n",
        "    layer = Embedding(\n",
        "        input_dim=len(lstm_data['tokenizer'].word_index)+1,\n",
        "        output_dim=SEQUENCE_MAX_LEN,\n",
        "        weights=[lstm_embedding_matrix],\n",
        "        input_length=MAX_N_WORDS,\n",
        "        trainable=False)(input_layer)\n",
        "\n",
        "    ################ PUT HERE THE DESIRED ARCHITECTURE ################\n",
        "    # E.g.: a two bidirectional LSTM with two layers of 128 neurons\n",
        "    layer = Bidirectional(LSTM(units=128, return_sequences=True))(layer)\n",
        "    layer = Bidirectional(LSTM(units=128))(layer)\n",
        "\n",
        "    layer = Dense(\n",
        "        name='output',\n",
        "        units=1)(layer)\n",
        "\n",
        "    output_layer = Activation(activation='sigmoid')(layer)\n",
        "    ################ PUT HERE THE DESIRED ARCHITECTURE ################\n",
        "\n",
        "    # Create an object for the model\n",
        "    lstm_model = Model(\n",
        "        inputs=input_layer,\n",
        "        outputs=output_layer)\n",
        "\n",
        "    # Compile the model \n",
        "    lstm_model.compile(\n",
        "        loss=LOSS_FUNCTION,\n",
        "        optimizer=OPTIMIZER,\n",
        "        metrics=VALID_METRICS)\n",
        "\n",
        "    # Train the built model\n",
        "    lstm_model.fit(\n",
        "        x=lstm_data['train_matrix'], \n",
        "        y=np.array(lstm_data['train_labels']),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        epochs=N_EPOCHS,\n",
        "        validation_split=VALID_RATE,\n",
        "        callbacks=MODEL_CALLBACKS,\n",
        "        verbose=0)\n",
        "\n",
        "    # Evaluate the trained LSTM model over train and test datasets\n",
        "    validate_lstm_model(\n",
        "        model=lstm_model,\n",
        "        train_matrix=lstm_data['train_matrix'],\n",
        "        train_labels=lstm_data['train_labels'],\n",
        "        test_matrix=lstm_data['test_matrix'],\n",
        "        test_labels=lstm_data['test_labels']\n",
        "    )\n",
        "\n",
        "## FINISH MEASURING TIME\n",
        "end = time.time()\n",
        "\n",
        "# Calculate the average of each metric\n",
        "print(f'Avg train acc: {round(sum(train_accuracy_values)/len(train_accuracy_values), 3)}') \n",
        "print(f'Avg train auc: {round(sum(train_auc_values)/len(train_auc_values), 3)}') \n",
        "print(f'Avg test acc: {round(sum(test_accuracy_values)/len(test_accuracy_values), 3)}') \n",
        "print(f'Avg test auc: {round(sum(test_auc_values)/len(test_auc_values), 3)}') \n",
        "\n",
        "# Add the execution time\n",
        "print(f'Total time: {round(((end-start)/60), 3)} min') "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "00f7dc61815a6da5453fee0a1d7c3baaa88d552412e55cf65ecdf10d17265d5d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
