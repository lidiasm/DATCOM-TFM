{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos LSTM\n",
    "\n",
    "## 1. Introducción a LSTM\n",
    "\n",
    "LSTM es un acrónimo de *Long-Short Term Memory* y representa a un **subtipo de RNN** (*Recurrent Neural Network*) capaz de **retener información relevante** sobre datos ya procesados que ayude al procesamiento de nuevas secuencias de datos completas. Su arquitectura se encuentra compuesta a su vez por tres redes neuronales:\n",
    "\n",
    "* ***Forget Gate***: este primer modelo es el encargado de filtrar qué información previa es útil para su almacenamiento y qué datos ya no son útiles para futuras iteraciones. \n",
    "\n",
    "* ***Input Gate***: esta segunda red trata de determinar el valor que presentan los datos entrantes para resolver la tarea de clasificación.\n",
    "\n",
    "* ***Output Gate***: finalmente esta red calcula las salidas del modelo LSTM que dependerán de la tarea de clasificación que se pretende abordar.\n",
    "\n",
    "### 1.1. Introducción a BiLSTM\n",
    "\n",
    "Se trata de una variante de la arquitectura compuesta por **dos redes LSTM independientes** con el objetivo de procesar los textos de derecha a izquierda y viceversa. Esta característica permite la **extracción de características en ambos sentidos** proporcionando un **contexto más voluminoso y preciso** al considerar los **términos precedores y sucesores**, almacenando así información pasada y futura del texto. Así, generalmente las redes BiLSTM tienden a mejorar el rendimiento y su capacidad preditiva.\n",
    "\n",
    "### 1.2. Condiciones de uso\n",
    "\n",
    "Dependiendo del framework que se pretenda utilizar (Tensorflow, Keras, Pytorch) existen diferentes tratamientos de datos y requisitos de implementación que se deben cumplir al definir la arquitectura, entrenamiento y validación de modelos. En mi caso particular he optado por utilizar **Keras** debido a la experiencia previa que tengo con la librería y a su facilidad de uso. \n",
    "\n",
    "1. **Procesamiento y limpieza** de los documentos.\n",
    "\n",
    "2. **Tokenización** de los documentos especificando un token para aquellos términos que no sean reconocidos dentro de un vocabulario de palabras.\n",
    "\n",
    "3. **Codificación** numérica en forma de matrices secuenciales de valores. \n",
    "\n",
    "5. **Normalización** de las secuencias numéricas para establecer un mismo **tamaño fijo**, completando con ceros aquellas de menor longitud y separando en varias secuencias aquellas que dispongan de un mayor tamaño.\n",
    "\n",
    "6. Definición de la arquitectura de un **modelo** e instanciación para su posterior entrenamiento y validación.\n",
    "\n",
    "### 1.3. Casos de uso\n",
    "\n",
    "* Detección y extracción de patrones en secuencias de datos.\n",
    "* Modelado del lenguaje natural.\n",
    "* Traducción de texto.\n",
    "* Reconocimiento de textos manuscritos.\n",
    "* Generación de imágenes mediante mecanismos de atención.\n",
    "* Sistemas de preguntas y respuestas.\n",
    "* Conversión de vídeo a texto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estructura del notebook\n",
    "\n",
    "1. Introducción a LSTM\n",
    "2. Estructura del notebook\n",
    "3. Instalación y carga de librerías\n",
    "4. Lectura y carga de datos\n",
    "5. Experimentos y modelos\n",
    "6. Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instalación y carga de librerías\n",
    "\n",
    "Este apartado tiene como único propósito cargar las librerías y dependencias necesarias para la ejecución de este notebook, así como las funciones propiamente desarrolladas. Previo a ello deberán ser instaladas bien ejecutando el script *setup.sh* mediante el comando `bash setup.sh` con permisos de ejecución en distribuciones Linux, o bien ejecutando el compando `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 12:43:15.221294: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 12:43:15.643914: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:43:15.643966: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-16 12:43:16.833250: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:43:16.833334: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:43:16.833341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-16 12:43:19.393628: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-16 12:43:19.393874: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-16 12:43:19.393893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lidiasm): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "# Import data read and compute functions\n",
    "from data import read_train_dataset, read_test_dataset\n",
    "\n",
    "# Import text preprocess functions\n",
    "from processing import *\n",
    "\n",
    "# numpy: to work with numeric codifications and embeddings\n",
    "import numpy as np\n",
    "\n",
    "# keras: to define and build LSTM models\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import LSTM, Activation, Dense, Input, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# sklearn: to plot a confusion matrix per trained model\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# matplotlib: to plot charts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lectura y carga de datos originales\n",
    "\n",
    "En esta sección se pretende **cargar los datasets de entrenamiento y validación** procedentes de los correspondientes ficheros situados en la carpeta *data*. Al tener un **formato TSV** se deben leer como tablas aunque posteriormente se trabaje con ellos en formato *dataframe*. \n",
    "\n",
    "Tal y como se puede comprobar en los siguientes resultados las dimensiones de sendos conjuntos de datos se detallan a continuación:\n",
    "\n",
    "* Conjunto de entrenamiento: **6.977 muestras**.\n",
    "* Conjunto de validación: **4.368 muestras**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset dimensions: (6977, 7)\n",
      "Test dataset dimensions: (4368, 7)\n"
     ]
    }
   ],
   "source": [
    "# Read EXIST datasets\n",
    "train_df = read_train_dataset()\n",
    "test_df = read_test_dataset()\n",
    "\n",
    "# Show the dimensions of the datasets\n",
    "print(\"Train dataset dimensions:\", train_df.shape)\n",
    "print(\"Test dataset dimensions:\", test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experimentos y modelos\n",
    "\n",
    "Esta sección pretende detallar los experimentos que se realizan a través de la combinación de diferentes técnicas de procesamiento de textos, codificación de documentos y arquitecturas de modelos LSTM. Como se trata de **experimentos no determinísticos**, es decir, los resultados difieren en varias ejecuciones aún con la misma configuración, la estrategia a seguir consiste en realizar **30 iteraciones de cada experimento** para luego calcular la **media de accuracy y AUC**, las métricas de evaluación escogidas para medir la calidad de un clasificador. \n",
    "\n",
    "Por lo tanto las siguientes secciones contienen los detalles del conjunto de experimentos realizados y las conclusiones comparativas alcanzadas, incluyendo el código, la configuración y los resultados únicamente del experimento con mejor rendimiento con respecto a las métricas de evaluación mencionadas.\n",
    "\n",
    "Previo al comienzo de la experimentación se definen tres funciones comunes para el tratamiento y codificación de documentos, carga de embeddings pre-entrenados y validación de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_matrix(\n",
    "    max_n_words: int, sequence_len: int, \n",
    "    lemm: bool = False, stemm: bool = False):\n",
    "    \"\"\"\n",
    "    Process the train and test documents to then convert them\n",
    "    into numeric sequence matrixes so the datasets can be\n",
    "    used to train a LSTM model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_n_words : int\n",
    "        Maximum number of words to keep within the LSTM memory\n",
    "        based on computing the word frequency.\n",
    "    sequence_len : int\n",
    "        Maximum lenght of all sequences.\n",
    "    lemm : bool (optional)\n",
    "        True to apply lemmatization to the train and test documents.\n",
    "    stemm : bool (optional)\n",
    "        True to apply stemming to the train and test documents.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary with the following keys:\n",
    "        - 'tokenizer': a Keras Tokenizer object based on the train documents\n",
    "        that contains the vocabulary to then be used to create the embeddings.\n",
    "        - 'train_matrix', 'test_matrix': the numeric sequence matrixes\n",
    "        after converting the train and test documents.\n",
    "        - 'train_labels', 'test_labels': two numeric lists which contains\n",
    "        the encoded class labels for train and test datasets.\n",
    "    \"\"\"\n",
    "    # Process train and test text documents\n",
    "    processed_df = process_encode_datasets(\n",
    "        train_df=train_df, \n",
    "        test_df=test_df,\n",
    "        lemm=lemm, \n",
    "        stemm=stemm,\n",
    "        correct_words=False\n",
    "    )\n",
    "\n",
    "    # Processed train texts and encoded train labels \n",
    "    train_texts = list(processed_df[\"train_df\"][\"cleaned_text\"].values)\n",
    "    train_labels = processed_df[\"encoded_train_labels\"]\n",
    "\n",
    "    # Processed test texts and encoded test labels\n",
    "    test_texts = list(processed_df[\"test_df\"][\"cleaned_text\"].values)\n",
    "    test_labels = processed_df[\"encoded_test_labels\"]\n",
    "\n",
    "    # Createa a tokenizer based on train texts\n",
    "    tokenizer = Tokenizer(num_words=max_n_words)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Transform each text into a numeric sequence\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "\n",
    "    # Transform each numeric sequence into a 2D vector\n",
    "    train_matrix = pad_sequences(\n",
    "        sequences=train_sequences, \n",
    "        maxlen=sequence_len)\n",
    "\n",
    "    # Tokenize the test documents using the prior trained tokenizer\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "    # Transform each numeric sequence into a 2D vector\n",
    "    test_matrix = pad_sequences(\n",
    "        sequences=test_sequences,\n",
    "        maxlen=sequence_len)\n",
    "\n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"train_matrix\": train_matrix,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"test_matrix\": test_matrix,\n",
    "        \"test_labels\": test_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_file: str, tokenizer: Tokenizer, sequence_len: int):\n",
    "    \"\"\"\n",
    "    Load the embeddings stored in the provided file to then\n",
    "    create a matrix with the numeric encoding of each\n",
    "    available word within the tokenizer vocabulary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_file : str\n",
    "        The path to the file which contains a set of embeddings\n",
    "    tokenizer : Tokenizer (Keras)\n",
    "        A trained Keras tokenizer which contains the vocabulary\n",
    "        of the documents to use during the training of models\n",
    "    sequence_len : int\n",
    "        Maximum lenght of all embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Numpy ndarray which represents an embedding matrix.\n",
    "    \"\"\"\n",
    "    # Load the embeddings stored in a TXT file\n",
    "    embedding_file = open(embedding_file)\n",
    "\n",
    "    # Store each word with its embeddings\n",
    "    embeddings_index = {\n",
    "        line.split()[0]:np.asarray(line.split()[1:], dtype=\"float32\") \n",
    "        for line in embedding_file\n",
    "    }\n",
    "\n",
    "    # Initialize the embedding matrix with zeros\n",
    "    embedding_matrix = np.zeros(shape=(len(tokenizer.word_index)+1, sequence_len))\n",
    "\n",
    "    # Complete the matrix with the prior loaded embeddings\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Search for the embeddings of each word\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "\n",
    "        # Words not found will be zeros\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lstm_model(\n",
    "    model: Model, \n",
    "    train_matrix: np.ndarray, train_labels: list, \n",
    "    test_matrix: np.ndarray, test_labels: list,\n",
    "    metrics_filename: str = None, conf_matrix_filename: str = None):\n",
    "    \"\"\"\n",
    "    Evaluates the provided trained LSTM model over the \n",
    "    train and test datasets to get the accuracy, AUC and\n",
    "    a confusion matrix. To create the predictions for a\n",
    "    binary classification a threshold has been set:\n",
    "        - <= 0.5 represents the negative class (non-sexist).\n",
    "        - > 0.5 represents the positive class (sexist).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Keras model\n",
    "        A trained Keras model to be evaluated.\n",
    "    train_matrix : Numpy ndarray\n",
    "        A numeric sequence matrix with the trained documents.\n",
    "    train_labels : list\n",
    "        A numeric list with the class labels of the train dataset.\n",
    "    test_matrix : Numpy ndarray\n",
    "        A numeric sequence matrix with the test documents.\n",
    "    test_labels : list\n",
    "        A numeric list with the class labels of the test dataset.\n",
    "    metrics_filename : str (optional)\n",
    "        A path and filename to save the metrics over the \n",
    "        train and test datasets in a TXT file.\n",
    "    conf_matrix_filename : str (optional)\n",
    "        A path and filename to save the confusion matrix in\n",
    "        a PNG image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    # Compute and print the accuracy and AUC over train\n",
    "    train_acc = model.evaluate(\n",
    "        x=train_matrix, \n",
    "        y=np.array(train_labels))\n",
    "\n",
    "    print(f\"Accuracy over train dataset: {train_acc[1]}\")\n",
    "    print(f\"AUC over train dataset: {train_acc[2]}\\n\")\n",
    "\n",
    "    # Compute and print the accuracy and AUC over test\n",
    "    test_acc = model.evaluate(\n",
    "        x=test_matrix, \n",
    "        y=np.array(test_labels))\n",
    "\n",
    "    print(f\"Accuracy over test dataset: {test_acc[1]}\")\n",
    "    print(f\"AUC over test dataset: {test_acc[2]}\")\n",
    "\n",
    "    # Generate class label predictions over the test dataset\n",
    "    # Class 0 ~ <= 0.5 | Class 1 ~ > 0.5\n",
    "    test_preds = (model.predict(test_matrix) > 0.5).astype(\"int32\")\n",
    "\n",
    "    # Plot the confusion matrix \n",
    "    ConfusionMatrixDisplay(\n",
    "        confusion_matrix=confusion_matrix(\n",
    "            np.array(test_labels), \n",
    "            np.array(test_preds)), \n",
    "        display_labels=[\"Non-sexist\", \"Sexist\"]) \\\n",
    "    .plot()    \n",
    "\n",
    "    # Save the confusion matrix in an image\n",
    "    if (type(conf_matrix_filename) == str and\n",
    "        len(conf_matrix_filename) > 0):\n",
    "        plt.savefig(conf_matrix_filename)\n",
    "\n",
    "    # Save the metrics in a text file\n",
    "    if (type(metrics_filename) == str and\n",
    "        len(metrics_filename) > 0):\n",
    "        opened_file = open(metrics_filename, \"w\")\n",
    "        print(f\"Accuracy over train dataset: {train_acc[1]}\", file=opened_file) \n",
    "        print(f\"AUC over train dataset: {train_acc[2]}\\n\", file=opened_file) \n",
    "        print(f\"Accuracy over test dataset: {test_acc[1]}\", file=opened_file) \n",
    "        print(f\"AUC over test dataset: {test_acc[2]}\", file=opened_file) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Mejor experimento: arquitectura LSTM\n",
    "\n",
    "Tras diversos experimentos con diferentes arquitecturas, embeddings, configuraciones de hiperparámetros y técnicas de procesamiento y aumento de textos, a continuación se presenta el modelo que mejores resultados ha proporcionado en relación a las métricas consideradas de *accuracy* y AUC. \n",
    "\n",
    "* **Procesamiento de los textos de entrenamiento y validación**.\n",
    "  * **Considera textos tanto en inglés como en español**.\n",
    "  * Elimina usuarios mencionados.\n",
    "  * Elimina caracteres no alfabéticos.\n",
    "  * Elimina *stopwords* en inglés y español.\n",
    "  * Elimina palabras sin vocales.\n",
    "  * <p>Convierte todos los caracteres a minúsculas.</p>\n",
    "\n",
    "* **Codificación de textos mediante embeddings**. Tras experimentar con diferentes ficheros de **Glove embeddings** encontrados en este [link](https://nlp.stanford.edu/projects/glove/), destacando que la gran mayoría únicamente disponen de términos en inglés, se ha empleado el fichero denominado **glove.twitter.27B.100d.txt** por **multilingüismo** y mayor capacidad de representación por el mayor número de términos que contiene. \n",
    "\n",
    "* **Arquitectura e hiperparámetros**.\n",
    "  * 1 capa de entrada para proporcionar los documentos procesados.\n",
    "  * **1 capa oculta con 128 neuronas**.\n",
    "  * 1 capa de salida con la que asignar una clase a cada muestra.\n",
    "  * **Tamaño del lote: 32**.\n",
    "  * <p>Máximo número de palabras que se mantienen en memoria: 1.000.</p>\n",
    "\n",
    "* **Entrenamiento y validación**.\n",
    "  * Número máximo de **iteraciones**: 100.\n",
    "  * **Early Stopping** tras 15 iteraciones sin mejorar el valor de la métrica *AUC* en validación y recuperando los pesos del mejor modelo encontrado. \n",
    "  * Porcentaje de **validación**: 20%.\n",
    "  * Función de pérdida: *binary_crossentropy*.\n",
    "  * Optimizador: Adam.\n",
    "  * **Métricas de validación**: *accuracy* y AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix:\n",
      "[[  0   0   0 ... 211 776  93]\n",
      " [  0   0   0 ... 142 455 856]\n",
      " [  0   0   0 ... 256 257 186]\n",
      " ...\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...  53 327 518]\n",
      " [  0   0   0 ...   2 742   8]]\n",
      "Train labels:[1, 0, 1, 0, 0]\n",
      "\n",
      "Test matrix:\n",
      "[[  0   0   0 ... 259 299  32]\n",
      " [  0   0   0 ... 109   3  48]\n",
      " [  0   0   0 ... 974 180 974]\n",
      " ...\n",
      " [  0   0   0 ... 512 420  97]\n",
      " [  0   0   0 ...   8  37 462]\n",
      " [  0   0   0 ...   5 102 167]]\n",
      "Test labels:[0, 0, 1, 1, 0]\n",
      "\n",
      "Glove Twitter 27B 100d embedding matrix:\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.18897    -1.25849998 -0.32616001 ... -0.04941     0.32721001\n",
      "   0.24265   ]\n",
      " [-0.12749    -0.62423998  0.26901001 ...  0.56253999  0.62739998\n",
      "   0.23934001]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.48534     0.38064    -1.05149996 ... -0.12978999 -0.26934001\n",
      "   0.57903999]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "MAX_N_WORDS = 1000\n",
    "SEQUENCE_MAX_LEN = 100\n",
    "EMBEDDING_FILE_PATH = \"../en_es_embeddings/glove.twitter.27B.100d.txt\"\n",
    "APPLY_LEMMATIZATION = False \n",
    "APPLY_STEMMING = False \n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 100\n",
    "VALID_RATE = 0.2\n",
    "MODEL_CALLBACKS = [EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    min_delta=0.001,\n",
    "    patience=15,\n",
    "    restore_best_weights=True)]\n",
    "LOSS_FUNCTION = \"binary_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "VALID_METRICS = [\"accuracy\", \"AUC\"]\n",
    "\n",
    "# Process the train and test documents as well as create\n",
    "# a tokenizer based on the processed train documents\n",
    "processed_data = get_train_test_matrix(\n",
    "    max_n_words=MAX_N_WORDS,\n",
    "    sequence_len=SEQUENCE_MAX_LEN,\n",
    "    lemm=False,\n",
    "    stemm=False\n",
    ")\n",
    "print(f\"Train matrix:\\n{processed_data['train_matrix']}\")\n",
    "print(f\"Train labels:{processed_data['train_labels'][0:5]}\")\n",
    "\n",
    "print(f\"\\nTest matrix:\\n{processed_data['test_matrix']}\")\n",
    "print(f\"Test labels:{processed_data['test_labels'][0:5]}\")\n",
    "\n",
    "# Load the embeddings stored in the defined file path\n",
    "# Encode the train matrix with these embeddings\n",
    "embedding_matrix = get_embedding_matrix(\n",
    "    embedding_file=EMBEDDING_FILE_PATH,\n",
    "    tokenizer=processed_data[\"tokenizer\"],\n",
    "    sequence_len=SEQUENCE_MAX_LEN\n",
    ")\n",
    "print(f\"\\nGlove Twitter 27B 100d embedding matrix:\\n{embedding_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 100, 100)          2515000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,632,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 2,515,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "175/175 [==============================] - 11s 57ms/step - loss: 0.6149 - accuracy: 0.6683 - auc: 0.7136 - val_loss: 0.7681 - val_accuracy: 0.5330 - val_auc: 0.6220\n",
      "Epoch 2/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.5565 - accuracy: 0.7221 - auc: 0.7845 - val_loss: 0.8231 - val_accuracy: 0.5444 - val_auc: 0.6378\n",
      "Epoch 3/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.5292 - accuracy: 0.7431 - auc: 0.8095 - val_loss: 0.8049 - val_accuracy: 0.5501 - val_auc: 0.6482\n",
      "Epoch 4/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.5042 - accuracy: 0.7599 - auc: 0.8302 - val_loss: 0.7756 - val_accuracy: 0.6024 - val_auc: 0.6552\n",
      "Epoch 5/100\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.4759 - accuracy: 0.7794 - auc: 0.8512 - val_loss: 0.8726 - val_accuracy: 0.5566 - val_auc: 0.6505\n",
      "Epoch 6/100\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 0.4518 - accuracy: 0.7950 - auc: 0.8674 - val_loss: 0.8358 - val_accuracy: 0.5852 - val_auc: 0.6592\n",
      "Epoch 7/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.4245 - accuracy: 0.8092 - auc: 0.8841 - val_loss: 1.0591 - val_accuracy: 0.5415 - val_auc: 0.6443\n",
      "Epoch 8/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.3894 - accuracy: 0.8240 - auc: 0.9041 - val_loss: 0.8999 - val_accuracy: 0.6010 - val_auc: 0.6513\n",
      "Epoch 9/100\n",
      "175/175 [==============================] - 13s 74ms/step - loss: 0.3491 - accuracy: 0.8481 - auc: 0.9249 - val_loss: 1.1826 - val_accuracy: 0.5559 - val_auc: 0.6451\n",
      "Epoch 10/100\n",
      "175/175 [==============================] - 13s 74ms/step - loss: 0.3089 - accuracy: 0.8724 - auc: 0.9422 - val_loss: 1.1398 - val_accuracy: 0.6003 - val_auc: 0.6607\n",
      "Epoch 11/100\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 0.2697 - accuracy: 0.8860 - auc: 0.9566 - val_loss: 1.3465 - val_accuracy: 0.5924 - val_auc: 0.6481\n",
      "Epoch 12/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.2339 - accuracy: 0.9007 - auc: 0.9681 - val_loss: 1.4071 - val_accuracy: 0.5809 - val_auc: 0.6454\n",
      "Epoch 13/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.1981 - accuracy: 0.9242 - auc: 0.9778 - val_loss: 1.4185 - val_accuracy: 0.5881 - val_auc: 0.6375\n",
      "Epoch 14/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.1655 - accuracy: 0.9378 - auc: 0.9852 - val_loss: 1.6623 - val_accuracy: 0.5888 - val_auc: 0.6404\n",
      "Epoch 15/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.1393 - accuracy: 0.9464 - auc: 0.9896 - val_loss: 1.8206 - val_accuracy: 0.5938 - val_auc: 0.6441\n",
      "Epoch 16/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.1242 - accuracy: 0.9534 - auc: 0.9919 - val_loss: 1.7458 - val_accuracy: 0.5996 - val_auc: 0.6471\n",
      "Epoch 17/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.1023 - accuracy: 0.9629 - auc: 0.9945 - val_loss: 1.6872 - val_accuracy: 0.6024 - val_auc: 0.6371\n",
      "Epoch 18/100\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 0.1015 - accuracy: 0.9618 - auc: 0.9945 - val_loss: 1.7579 - val_accuracy: 0.5989 - val_auc: 0.6424\n",
      "Epoch 19/100\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 0.1170 - accuracy: 0.9593 - auc: 0.9923 - val_loss: 1.9262 - val_accuracy: 0.5867 - val_auc: 0.6523\n",
      "Epoch 20/100\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.0825 - accuracy: 0.9706 - auc: 0.9966 - val_loss: 2.2939 - val_accuracy: 0.5616 - val_auc: 0.6417\n",
      "Epoch 21/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0754 - accuracy: 0.9737 - auc: 0.9970 - val_loss: 1.9994 - val_accuracy: 0.5788 - val_auc: 0.6438\n",
      "Epoch 22/100\n",
      "175/175 [==============================] - 13s 74ms/step - loss: 0.0661 - accuracy: 0.9758 - auc: 0.9978 - val_loss: 1.9608 - val_accuracy: 0.5917 - val_auc: 0.6426\n",
      "Epoch 23/100\n",
      "175/175 [==============================] - 13s 76ms/step - loss: 0.0598 - accuracy: 0.9772 - auc: 0.9981 - val_loss: 2.0730 - val_accuracy: 0.5910 - val_auc: 0.6425\n",
      "Epoch 24/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0560 - accuracy: 0.9776 - auc: 0.9982 - val_loss: 2.2295 - val_accuracy: 0.5795 - val_auc: 0.6403\n",
      "Epoch 25/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0511 - accuracy: 0.9799 - auc: 0.9986 - val_loss: 2.4912 - val_accuracy: 0.5716 - val_auc: 0.6375\n",
      "Epoch 26/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0496 - accuracy: 0.9805 - auc: 0.9986 - val_loss: 2.1900 - val_accuracy: 0.5910 - val_auc: 0.6400\n",
      "Epoch 27/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0476 - accuracy: 0.9803 - auc: 0.9986 - val_loss: 2.4199 - val_accuracy: 0.5781 - val_auc: 0.6445\n",
      "Epoch 28/100\n",
      "175/175 [==============================] - 13s 73ms/step - loss: 0.0470 - accuracy: 0.9806 - auc: 0.9986 - val_loss: 2.6812 - val_accuracy: 0.5638 - val_auc: 0.6306\n",
      "Epoch 29/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.1037 - accuracy: 0.9595 - auc: 0.9927 - val_loss: 2.2420 - val_accuracy: 0.5766 - val_auc: 0.6298\n",
      "Epoch 30/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0870 - accuracy: 0.9654 - auc: 0.9954 - val_loss: 2.1394 - val_accuracy: 0.5809 - val_auc: 0.6370\n",
      "Epoch 31/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0760 - accuracy: 0.9737 - auc: 0.9961 - val_loss: 2.2718 - val_accuracy: 0.5788 - val_auc: 0.6433\n",
      "Epoch 32/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.0503 - accuracy: 0.9801 - auc: 0.9984 - val_loss: 2.1289 - val_accuracy: 0.5924 - val_auc: 0.6413\n",
      "Epoch 33/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.0429 - accuracy: 0.9814 - auc: 0.9989 - val_loss: 2.3319 - val_accuracy: 0.5917 - val_auc: 0.6471\n",
      "Epoch 34/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0412 - accuracy: 0.9817 - auc: 0.9990 - val_loss: 2.5567 - val_accuracy: 0.5688 - val_auc: 0.6409\n",
      "Epoch 35/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0409 - accuracy: 0.9812 - auc: 0.9990 - val_loss: 2.4039 - val_accuracy: 0.5917 - val_auc: 0.6456\n",
      "Epoch 36/100\n",
      "175/175 [==============================] - 11s 62ms/step - loss: 0.0396 - accuracy: 0.9812 - auc: 0.9990 - val_loss: 2.5539 - val_accuracy: 0.5852 - val_auc: 0.6468\n",
      "Epoch 37/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0402 - accuracy: 0.9815 - auc: 0.9989 - val_loss: 2.5017 - val_accuracy: 0.6046 - val_auc: 0.6464\n",
      "Epoch 38/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0393 - accuracy: 0.9828 - auc: 0.9989 - val_loss: 2.5619 - val_accuracy: 0.5881 - val_auc: 0.6449\n",
      "Epoch 39/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0387 - accuracy: 0.9819 - auc: 0.9989 - val_loss: 2.7360 - val_accuracy: 0.5781 - val_auc: 0.6410\n",
      "Epoch 40/100\n",
      "175/175 [==============================] - 14s 79ms/step - loss: 0.0391 - accuracy: 0.9823 - auc: 0.9989 - val_loss: 2.7070 - val_accuracy: 0.5910 - val_auc: 0.6450\n",
      "Epoch 41/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.0405 - accuracy: 0.9817 - auc: 0.9989 - val_loss: 2.7698 - val_accuracy: 0.5809 - val_auc: 0.6427\n",
      "Epoch 42/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0394 - accuracy: 0.9805 - auc: 0.9989 - val_loss: 2.7591 - val_accuracy: 0.5795 - val_auc: 0.6430\n",
      "Epoch 43/100\n",
      "175/175 [==============================] - 14s 81ms/step - loss: 0.0382 - accuracy: 0.9826 - auc: 0.9989 - val_loss: 2.7739 - val_accuracy: 0.5867 - val_auc: 0.6452\n",
      "Epoch 44/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.0382 - accuracy: 0.9821 - auc: 0.9989 - val_loss: 2.7533 - val_accuracy: 0.5946 - val_auc: 0.6423\n",
      "Epoch 45/100\n",
      "175/175 [==============================] - 13s 73ms/step - loss: 0.0375 - accuracy: 0.9819 - auc: 0.9989 - val_loss: 2.7174 - val_accuracy: 0.5917 - val_auc: 0.6448\n",
      "Epoch 46/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.0372 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.9083 - val_accuracy: 0.5931 - val_auc: 0.6427\n",
      "Epoch 47/100\n",
      "175/175 [==============================] - 13s 73ms/step - loss: 0.0383 - accuracy: 0.9819 - auc: 0.9989 - val_loss: 2.9545 - val_accuracy: 0.5752 - val_auc: 0.6410\n",
      "Epoch 48/100\n",
      "175/175 [==============================] - 13s 77ms/step - loss: 0.0904 - accuracy: 0.9643 - auc: 0.9944 - val_loss: 2.4219 - val_accuracy: 0.5595 - val_auc: 0.6390\n",
      "Epoch 49/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.0867 - accuracy: 0.9658 - auc: 0.9951 - val_loss: 2.3471 - val_accuracy: 0.5652 - val_auc: 0.6357\n",
      "Epoch 50/100\n",
      "175/175 [==============================] - 13s 74ms/step - loss: 0.0450 - accuracy: 0.9814 - auc: 0.9988 - val_loss: 2.2943 - val_accuracy: 0.5852 - val_auc: 0.6458\n",
      "Epoch 51/100\n",
      "175/175 [==============================] - 13s 75ms/step - loss: 0.0372 - accuracy: 0.9833 - auc: 0.9990 - val_loss: 2.3483 - val_accuracy: 0.5996 - val_auc: 0.6433\n",
      "Epoch 52/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.0366 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.5526 - val_accuracy: 0.5802 - val_auc: 0.6447\n",
      "Epoch 53/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.0356 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.6152 - val_accuracy: 0.5852 - val_auc: 0.6433\n",
      "Epoch 54/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0363 - accuracy: 0.9824 - auc: 0.9990 - val_loss: 2.5054 - val_accuracy: 0.6010 - val_auc: 0.6442\n",
      "Epoch 55/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0360 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.5601 - val_accuracy: 0.5981 - val_auc: 0.6430\n",
      "Epoch 56/100\n",
      "175/175 [==============================] - 12s 68ms/step - loss: 0.0351 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.7512 - val_accuracy: 0.5759 - val_auc: 0.6413\n",
      "Epoch 57/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0349 - accuracy: 0.9821 - auc: 0.9991 - val_loss: 2.7226 - val_accuracy: 0.5924 - val_auc: 0.6446\n",
      "Epoch 58/100\n",
      "175/175 [==============================] - 12s 70ms/step - loss: 0.0363 - accuracy: 0.9817 - auc: 0.9991 - val_loss: 2.7366 - val_accuracy: 0.5723 - val_auc: 0.6418\n",
      "Epoch 59/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0355 - accuracy: 0.9824 - auc: 0.9990 - val_loss: 2.7348 - val_accuracy: 0.5817 - val_auc: 0.6452\n",
      "Epoch 60/100\n",
      "175/175 [==============================] - 14s 78ms/step - loss: 0.0360 - accuracy: 0.9815 - auc: 0.9990 - val_loss: 2.7383 - val_accuracy: 0.5917 - val_auc: 0.6429\n",
      "Epoch 61/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0356 - accuracy: 0.9821 - auc: 0.9991 - val_loss: 2.7340 - val_accuracy: 0.5881 - val_auc: 0.6444\n",
      "Epoch 62/100\n",
      "175/175 [==============================] - 14s 82ms/step - loss: 0.0354 - accuracy: 0.9828 - auc: 0.9990 - val_loss: 2.8938 - val_accuracy: 0.5831 - val_auc: 0.6429\n",
      "Epoch 63/100\n",
      "175/175 [==============================] - 13s 72ms/step - loss: 0.0359 - accuracy: 0.9821 - auc: 0.9989 - val_loss: 2.7455 - val_accuracy: 0.6053 - val_auc: 0.6438\n",
      "Epoch 64/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0363 - accuracy: 0.9815 - auc: 0.9990 - val_loss: 2.8112 - val_accuracy: 0.5817 - val_auc: 0.6460\n",
      "Epoch 65/100\n",
      "175/175 [==============================] - 11s 65ms/step - loss: 0.0356 - accuracy: 0.9817 - auc: 0.9990 - val_loss: 2.8130 - val_accuracy: 0.5795 - val_auc: 0.6427\n",
      "Epoch 66/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0347 - accuracy: 0.9833 - auc: 0.9990 - val_loss: 2.9877 - val_accuracy: 0.5809 - val_auc: 0.6418\n",
      "Epoch 67/100\n",
      "175/175 [==============================] - 12s 71ms/step - loss: 0.0363 - accuracy: 0.9830 - auc: 0.9991 - val_loss: 2.6212 - val_accuracy: 0.6089 - val_auc: 0.6482\n",
      "Epoch 68/100\n",
      "175/175 [==============================] - 12s 69ms/step - loss: 0.0366 - accuracy: 0.9815 - auc: 0.9989 - val_loss: 2.8604 - val_accuracy: 0.5874 - val_auc: 0.6447\n",
      "Epoch 69/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0356 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.8011 - val_accuracy: 0.6067 - val_auc: 0.6427\n",
      "Epoch 70/100\n",
      "175/175 [==============================] - 12s 66ms/step - loss: 0.0352 - accuracy: 0.9821 - auc: 0.9990 - val_loss: 2.9678 - val_accuracy: 0.5881 - val_auc: 0.6423\n",
      "Epoch 71/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0357 - accuracy: 0.9814 - auc: 0.9990 - val_loss: 2.9203 - val_accuracy: 0.5874 - val_auc: 0.6428\n",
      "Epoch 72/100\n",
      "175/175 [==============================] - 11s 66ms/step - loss: 0.0353 - accuracy: 0.9823 - auc: 0.9991 - val_loss: 2.9927 - val_accuracy: 0.5752 - val_auc: 0.6408\n",
      "Epoch 73/100\n",
      "175/175 [==============================] - 12s 67ms/step - loss: 0.0348 - accuracy: 0.9832 - auc: 0.9990 - val_loss: 3.2195 - val_accuracy: 0.5788 - val_auc: 0.6376\n",
      "Epoch 74/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.1271 - accuracy: 0.9654 - auc: 0.9901 - val_loss: 2.7237 - val_accuracy: 0.4821 - val_auc: 0.5723\n",
      "Epoch 75/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.2114 - accuracy: 0.9224 - auc: 0.9731 - val_loss: 1.8567 - val_accuracy: 0.5989 - val_auc: 0.6459\n",
      "Epoch 76/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0502 - accuracy: 0.9808 - auc: 0.9989 - val_loss: 2.0098 - val_accuracy: 0.5981 - val_auc: 0.6450\n",
      "Epoch 77/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0387 - accuracy: 0.9835 - auc: 0.9992 - val_loss: 2.2843 - val_accuracy: 0.5888 - val_auc: 0.6453\n",
      "Epoch 78/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0362 - accuracy: 0.9832 - auc: 0.9991 - val_loss: 2.3059 - val_accuracy: 0.5946 - val_auc: 0.6448\n",
      "Epoch 79/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0351 - accuracy: 0.9823 - auc: 0.9990 - val_loss: 2.4222 - val_accuracy: 0.5967 - val_auc: 0.6450\n",
      "Epoch 80/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0344 - accuracy: 0.9830 - auc: 0.9991 - val_loss: 2.4838 - val_accuracy: 0.5938 - val_auc: 0.6460\n",
      "Epoch 81/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0341 - accuracy: 0.9826 - auc: 0.9990 - val_loss: 2.5144 - val_accuracy: 0.5967 - val_auc: 0.6476\n",
      "Epoch 82/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0337 - accuracy: 0.9830 - auc: 0.9992 - val_loss: 2.6153 - val_accuracy: 0.5931 - val_auc: 0.6484\n",
      "Epoch 83/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0337 - accuracy: 0.9828 - auc: 0.9991 - val_loss: 2.5893 - val_accuracy: 0.5967 - val_auc: 0.6463\n",
      "Epoch 84/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0336 - accuracy: 0.9823 - auc: 0.9991 - val_loss: 2.7042 - val_accuracy: 0.5888 - val_auc: 0.6453\n",
      "Epoch 85/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0336 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.6809 - val_accuracy: 0.5974 - val_auc: 0.6484\n",
      "Epoch 86/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0334 - accuracy: 0.9832 - auc: 0.9991 - val_loss: 2.6525 - val_accuracy: 0.5989 - val_auc: 0.6442\n",
      "Epoch 87/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0337 - accuracy: 0.9832 - auc: 0.9991 - val_loss: 2.7810 - val_accuracy: 0.5903 - val_auc: 0.6452\n",
      "Epoch 88/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0358 - accuracy: 0.9812 - auc: 0.9991 - val_loss: 2.2823 - val_accuracy: 0.6132 - val_auc: 0.6510\n",
      "Epoch 89/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0477 - accuracy: 0.9772 - auc: 0.9983 - val_loss: 2.5599 - val_accuracy: 0.5967 - val_auc: 0.6422\n",
      "Epoch 90/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0417 - accuracy: 0.9808 - auc: 0.9988 - val_loss: 3.0019 - val_accuracy: 0.5666 - val_auc: 0.6386\n",
      "Epoch 91/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0343 - accuracy: 0.9817 - auc: 0.9991 - val_loss: 2.7726 - val_accuracy: 0.5931 - val_auc: 0.6431\n",
      "Epoch 92/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0339 - accuracy: 0.9828 - auc: 0.9991 - val_loss: 2.9530 - val_accuracy: 0.5795 - val_auc: 0.6432\n",
      "Epoch 93/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0338 - accuracy: 0.9819 - auc: 0.9990 - val_loss: 2.8039 - val_accuracy: 0.6017 - val_auc: 0.6431\n",
      "Epoch 94/100\n",
      "175/175 [==============================] - 11s 63ms/step - loss: 0.0332 - accuracy: 0.9832 - auc: 0.9991 - val_loss: 2.8940 - val_accuracy: 0.6010 - val_auc: 0.6441\n",
      "Epoch 95/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0343 - accuracy: 0.9830 - auc: 0.9990 - val_loss: 2.8863 - val_accuracy: 0.5996 - val_auc: 0.6403\n",
      "Epoch 96/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0331 - accuracy: 0.9824 - auc: 0.9991 - val_loss: 2.9274 - val_accuracy: 0.5981 - val_auc: 0.6385\n",
      "Epoch 97/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0345 - accuracy: 0.9815 - auc: 0.9990 - val_loss: 2.9111 - val_accuracy: 0.6039 - val_auc: 0.6442\n",
      "Epoch 98/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0333 - accuracy: 0.9826 - auc: 0.9991 - val_loss: 2.9784 - val_accuracy: 0.5989 - val_auc: 0.6376\n",
      "Epoch 99/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0330 - accuracy: 0.9832 - auc: 0.9991 - val_loss: 3.0973 - val_accuracy: 0.5867 - val_auc: 0.6429\n",
      "Epoch 100/100\n",
      "175/175 [==============================] - 11s 64ms/step - loss: 0.0335 - accuracy: 0.9828 - auc: 0.9990 - val_loss: 2.9850 - val_accuracy: 0.6017 - val_auc: 0.6414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d55af7be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input layer\n",
    "input_layer = Input(\n",
    "    name=\"inputs\",\n",
    "    shape=[SEQUENCE_MAX_LEN])\n",
    "\n",
    "## Embedding layer: pre-trained embeddings\n",
    "layer = Embedding(\n",
    "    input_dim=len(processed_data[\"tokenizer\"].word_index)+1,\n",
    "    output_dim=SEQUENCE_MAX_LEN,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_N_WORDS,\n",
    "    trainable=False)(input_layer)\n",
    "\n",
    "## LSTM layer\n",
    "layer = LSTM(units=128)(layer)\n",
    "\n",
    "## Output layer\n",
    "layer = Dense(\n",
    "    name=\"output\",\n",
    "    units=1)(layer)\n",
    "\n",
    "## Activation layer\n",
    "output_layer = Activation(activation=\"sigmoid\")(layer)\n",
    "\n",
    "## Model object\n",
    "lstm_model = Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=output_layer)\n",
    "\n",
    "## Summary of the model\n",
    "lstm_model.summary()\n",
    "\n",
    "## Compile the model \n",
    "lstm_model.compile(\n",
    "    loss=LOSS_FUNCTION,\n",
    "    optimizer=OPTIMIZER,\n",
    "    metrics=VALID_METRICS)\n",
    "\n",
    "## Train the built model\n",
    "lstm_model.fit(\n",
    "    x=processed_data[\"train_matrix\"], \n",
    "    y=np.array(processed_data[\"train_labels\"]),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=N_EPOCHS,\n",
    "    validation_split=VALID_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se puede apreciar en los siguientes resultados, la tasa de aciertos sobre el conjunto de entrenamiento es bastante alta con un 90% aunque sobre el conjunto de **test apenas alcanza el 67% de accuracy**. Considerando el ínfimo valor del área bajo la curva ROC en base a este conjunto de datos se puede concluir que la **capacidad de predicción del modelo no es aceptable**, siendo apenas superior a la que tendría un clasificador aleatorio. \n",
    "\n",
    "Observando la matriz de confusión es altamente notable la **elevada tasa de falsos negativos**, es decir, textos sexistas que no han sido detectados. Por lo tanto una arquitectura LSTM y la mejor configuración encontrada en este grupo de experimentos, parece no ser suficiente precisa como para construir un clasificador de calidad capaz de identificar textos sexistas y no sexistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 5s 22ms/step - loss: 0.6229 - accuracy: 0.9076 - auc: 0.9425\n",
      "Accuracy over train dataset: 0.9075533747673035\n",
      "AUC over train dataset: 0.9424617290496826\n",
      "\n",
      "137/137 [==============================] - 3s 22ms/step - loss: 2.1016 - accuracy: 0.6719 - auc: 0.7155\n",
      "Accuracy over test dataset: 0.6719322204589844\n",
      "AUC over test dataset: 0.7154521346092224\n",
      "137/137 [==============================] - 3s 21ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Evaluate the trained LSTM model over train and test datasets\n",
    "validate_lstm_model(\n",
    "    model=lstm_model,\n",
    "    train_matrix=processed_data[\"train_matrix\"],\n",
    "    train_labels=processed_data[\"train_labels\"],\n",
    "    test_matrix=processed_data[\"test_matrix\"],\n",
    "    test_labels=processed_data[\"test_labels\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Mejor experimento: arquitectura BiLSTM\n",
    "\n",
    "Apoyado en los experimentos y resultados con una arquitectura LSTM se ha podido descubrir que prácticamente la totalidad de las **configuraciones y conclusiones anteriores son perfectamente aplicables a un clasificador BiLSTM**. La única **excepción** existente es que en este caso resulta más ventajoso añadir **una capa oculta más con 128 neuronas** para aumentar las métricas de *accuracy* y AUC en aproximadamente un 2%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusiones de experimentos con LSTM y BiLSTM\n",
    "\n",
    "### 6.1. Técnicas de procesamiento de textos\n",
    "\n",
    "  * **Ni la lematización ni el stemming han conseguido apenas mejorar** la capacidad predictiva de los modelos aunque sí que aumenta el tiempo y recursos de computación, especialmente con la primera técnica. Por lo tanto en esta arquitectura de ejemplo no se aplican ninguno de los dos métodos.\n",
    "\n",
    "  * Otra técnica empleada ha sido la **detección y corrección de palabras mal escritas** puesto que las fuentes de datos de las que proceden los documentos son redes sociales y es bien conocido que en estos medios la escritura de textos no es ni precisa ni correcta en la mayoría de ocasiones. Si bien el objetivo era **aumentar el número de palabras codificables** por los *embeddings* incrementando la representatividad de las matrices de entrada a los modelos, **tampoco se ha conseguido incrementar la capacidad de predicción** de los modelos. Analizando las posibles teorías explicativas de este suceso, se han calculado las siguientes métricas para el conjunto de entrenamiento:\n",
    "\n",
    "    * Número de palabras totales: 201.361\n",
    "    * Número de términos incorrectos detectados: 19.008\n",
    "    * Número de términos incorrectos corregidos: 15.341\n",
    "\n",
    "    Tal y como se puede apreciar **únicamente existe un 10% aproximadamente de términos incorrectos** en los textos de entrenamiento aunque se han corregido más de un 80% de ellos. Dadas estas cifras, parece razonable el hecho de no notar mejoría alguna aplicando esta técnica para el entrenamiento de modelos LSTM.\n",
    "\n",
    "### 6.2. Embeddings\n",
    "\n",
    "Tras codificar los documentos de entrenamiento y validación con varios de los ficheros de **embeddings** relativos a este [link](https://nlp.stanford.edu/projects/glove/) utilizando la arquitectura explicada anteriormente, se ha podido determinar que aquellos **embeddings basados en un vocabulario más voluminoso** proporcionan mejores métricas de *accuracy* y AUC por aumentar la representatividad de los textos al **codificar un mayor número de palabras**. \n",
    "\n",
    "No obstante no ocurre lo mismo con el **número de vectores**, puesto que los ficheros de 200 y 300 disparan el tiempo de entrenamiento y validación mientras que los valores de las anteriores métricas se mantienen prácticamente invariables con respecto al uso del fichero con únicamente 100 vectores. \n",
    "\n",
    "### 6.3. Batch size\n",
    "\n",
    "En este apartado se destaca la experimentación con diferentes tamaños de lote, siendo los valores más comunes: **128, 64 y 32**. Según los resultados obtenidos **apenas existe diferencia entre los dos primeros** en relación a las métricas de *accuracy* y AUC, invirtiendo un menor número de recursos computacionales en el primer caso con una cifra mayor. Ha sido el **tamaño de lote 32** el que ha conseguido **mejorar las métricas de validación en más de un 2%** a costa del incremento del tiempo de entrenamiento y validación en menos de 5 minutos, por lo que es el que ha sido seleccionado para continuar con la experimentación.\n",
    "\n",
    "### 6.5. Configuración de entrenamiento\n",
    "\n",
    "La inclusión de la técnica **Early Stopping** ha sido **crucial para evitar el *overfitting*** y el malgasto de recursos y tiempo computacional para obtener un modelo con peor calidad a costa de un mayor número de iteraciones. Para su configuración se han probado diferentes métricas como *loss*, *accuracy* y ***AUC***, siendo esta última la elegida por demostrar un **equilibrio entre el tiempo invertido y su capacidad predictiva**, ya que con *accuracy* se conseguían clasificadores muy tempranos recortando considerablemente el número de iteraciones utilizadas para su construcción. \n",
    "\n",
    "### 6.6. Data Augmentation\n",
    "\n",
    "* **Traducción de textos**. Esta primera técnica se ha empleado para aumentar el número de muestras del conjunto de entrenamiento traduciendo los **textos ingleses a español y viceversa**, consiguiendo el doble de documentos. \n",
    "\n",
    "* **Easy Data Augmentation**. Conocida como EDA se trata de un conjunto de técnicas con las que se generan nuevos textos añadiendo **sinónimos** de palabras elegidas aleatoriamente, modificando la **posición** de términos de manera aleatoria e incluso **eliminando** conceptos con una cierta probabilidad aleatoria.\n",
    "\n",
    "* **Round-Trip Translation**. También denominada RTT es una metodología capaz de traducir documentos de su idioma origen a otro especificado para generar nuevos documentos a partir de una **segunda traducción a la inversa** aprovechando las variaciones incluidas en esta fase debido a la interpretación de los traductores. \n",
    "\n",
    "* **Contextual Word Embeddings**. Esta técnica emplea modelos de **parafraseo y/o transformers** para generar nuevos textos a partir del **contexto** de las muestras proporcionadas con el objetivo de mantener su significado y así proporcionar un conjunto de documentos más homogeneo.\n",
    "\n",
    "Si bien se han utilizado diferentes procesos con los que aumentar el conjunto de entrenamiento, no se ha podido descubrir ninguna mejora importante al construir modelos LSTM y BiLSTM.\n",
    "\n",
    "### 6.7. Arquitecturas\n",
    "\n",
    "Se han experimentado con diversas arquitecturas añadiendo/eliminando número de neuronas, capas ocultas, combinando capas bidireccionales con unidireccionales, añadiendo capas *drop out*, y a las conclusiones a las que se han podido alcanzar residen en que para el **conjunto de entrenamiento original**, es suficiente con un clasificador de **una capa oculta y 128 neuronas** ya que con arquitecturas más complejas no se consiguen aumentar las métricas de *accuracy* y AUC. Sin embargo, empleando **conjuntos de entrenamiento ampliados con documentos sintéticos**, sí se ha podido descubrir que es ligeramente beneficioso añadir un **mayor número de capas ocultas**. Así las dos arquitecturas que mejores resultados han proporcionado por detrás de la planteada en este notebook son:\n",
    "\n",
    "- Arquitectura de 2 capas ocultas con 128 neuronas cada una.\n",
    "- Arquitectura de 3 capas ocultas con 128, 64 y 32 neuronas, respectivamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00f7dc61815a6da5453fee0a1d7c3baaa88d552412e55cf65ecdf10d17265d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
